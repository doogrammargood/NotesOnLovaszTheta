#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\begin_preamble

\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Notes on Lovasz Theta
\end_layout

\begin_layout Author
Victor Bankston
\end_layout

\begin_layout Standard
The purpose of this example is to illustrate some of the structure to 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Notes.eps
	scale 60

\end_inset


\end_layout

\begin_layout Standard
Though the quantities above are scalars, they arise from arrangements of
 vectors.
 I will write down the vectors associated with each quantity above when
 
\begin_inset Formula $G$
\end_inset

 is the Petersen Graph.
\end_layout

\begin_layout Section*
Defining 
\begin_inset Formula $\vartheta$
\end_inset


\end_layout

\begin_layout Standard
First, we need a series of definitions to define 
\begin_inset Formula $\vartheta$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lov78"

\end_inset

:
\end_layout

\begin_layout Definition
Given a graph 
\begin_inset Formula $G$
\end_inset

, an orthonormal representation of 
\begin_inset Formula $G$
\end_inset

 is a mapping 
\begin_inset Formula $r:V\left(G\right)\to\mathbb{S}^{n}\subset\mathbb{R}^{n+1}$
\end_inset

 (for some 
\begin_inset Formula $n\in\mathbb{N}$
\end_inset

) such that if 
\begin_inset Formula $i\neq j\in V\left(G\right)$
\end_inset

, with 
\begin_inset Formula $i\not\sim j$
\end_inset

, then 
\begin_inset Formula $r\left(i\right)\perp r\left(j\right)$
\end_inset

.
 (Be careful: a vertex is not adjacent to itself).
\end_layout

\begin_layout Standard
Note that each graph has at least one representation, where 
\begin_inset Formula $v$
\end_inset

 maps the verticies each to its own orthonormal vector.
\end_layout

\begin_layout Definition
A valuation of an orthonormal representation 
\begin_inset Formula $val\left(r\right)$
\end_inset

 is 
\begin_inset Formula 
\[
\min_{\psi}\max_{v\in V\left(G\right)}\frac{1}{\left(\psi^{T}r\left(v\right)\right)^{2}}
\]

\end_inset


\end_layout

\begin_layout Definition
where 
\begin_inset Formula $\psi$
\end_inset

 ranges over all unit vectors (of the target space of 
\begin_inset Formula $r$
\end_inset

).
\end_layout

\begin_layout Standard
Given an orthonormal representation, its valuation is how tightly it can
 be embedded into a cone around some vector (
\begin_inset Formula $\psi$
\end_inset

).
\end_layout

\begin_layout Definition
Define 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 to be the minimum valuation over all orthonormal representations of 
\begin_inset Formula $G$
\end_inset

.
\end_layout

\begin_layout Standard
We can show that this minimum is actually attained.
 We will use Bolzano-Weirstrass.
 To see this, fix 
\begin_inset Formula $n$
\end_inset

, and consider the orthonormal representations of the form: 
\begin_inset Formula $v:V\left(G\right)\to\mathbb{S}^{n-1}\subset\mathbb{R}^{n}$
\end_inset

.
 Observe that 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 remains unchanged if we require that 
\begin_inset Formula $\psi=\left(1,0,0,0\dots\right)$
\end_inset

: These valuations are defined by an inner product, which will not change
 if we apply a fixed unitary 
\begin_inset Formula $U$
\end_inset

 to every vector.
 Choose 
\begin_inset Formula $U$
\end_inset

 to send 
\begin_inset Formula $\psi\mapsto\left(1,0,0,0\dots\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Fixing 
\begin_inset Formula $\psi$
\end_inset

, take a sequence of orthonormal representations whose values converge to
 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

.
 Observe that these orthonormal representations themselves can be considered
 as bounded vectors of dimension 
\begin_inset Formula $n\cdot V\left(G\right)$
\end_inset

, by concatenating all 
\begin_inset Formula $V\left(G\right)$
\end_inset

 vectors of dimension 
\begin_inset Formula $n$
\end_inset

.
 By the Bolzano-Weirstrass theorem, these have a convergent subsequence,
 so there is an accumulation point, 
\begin_inset Formula $r_{\infty}$
\end_inset

, which we must show is an orthonormal representation.
\end_layout

\begin_layout Standard
Our convergent subsequence of orthonormal representations gives rise to
 
\begin_inset Formula $V\left(G\right)$
\end_inset

 convergent sequences of vectors.
 We must show that each sequence of vectors goes to a unit vector, and that
 when 
\begin_inset Formula $i\not\sim j$
\end_inset

, with 
\begin_inset Formula $i\neq j$
\end_inset

, we have 
\begin_inset Formula $r_{\infty}\left(i\right)^{T}r_{\infty}\left(j\right)=0$
\end_inset

.
 Both of these are consequences of the fact that dot products are continuous:
 
\begin_inset Formula $0=\lim_{n\to\infty}r_{n}\left(i\right)^{T}r_{n}\left(j\right)=\left(\lim_{n\to\infty}r_{n}\left(i\right)\right)^{T}\left(\lim_{n\to\infty}r_{n}\left(j\right)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
There is no claim that such optimal representations are unique.
\end_layout

\begin_layout Section*
Graphs
\end_layout

\begin_layout Definition
Define the Kneser Graph 
\begin_inset Formula $k\left(n,r\right)$
\end_inset

 to have 
\begin_inset Formula ${n \choose r}$
\end_inset

 verticies labeled by 
\begin_inset Formula $r$
\end_inset

-element subsets from a universe of size 
\begin_inset Formula $n$
\end_inset

.
 Two verticies are adjacent if their corresponding sets are disjoint.
 We assume that 
\begin_inset Formula $n\geq2r$
\end_inset


\end_layout

\begin_layout Standard
Kneser graphs are vertex and edge transitive.
 Given any two verticies (edges), there is an automorphism which sends one
 to the other.
\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $G$
\end_inset

 is vertex and edge transitive, then 
\begin_inset Formula $\vartheta\left(G\right)\vartheta\left(\overline{G}\right)=n$
\end_inset

, and 
\begin_inset Formula $\vartheta\left(G\right)=\frac{-n\lambda_{n}}{\lambda_{1}-\lambda_{n}}$
\end_inset


\end_layout

\begin_layout Standard
This powerful theorem was originally used to find 
\begin_inset Formula $\vartheta\left(k\left(n,r\right)\right)$
\end_inset

.
 The proof of the theorem builds on the relations in the diagram.
\end_layout

\begin_layout Definition
The Petersen Graph, 
\begin_inset Formula $P$
\end_inset

, is the Kneser graph, 
\begin_inset Formula $k\left(5,2\right)$
\end_inset

.
\end_layout

\begin_layout Definition
\begin_inset Graphics
	filename 568px-Kneser_graph_KG(5,2).svg
	scale 30

\end_inset


\end_layout

\begin_layout Standard
We start with some graph properties.
\end_layout

\begin_layout Claim
The clique number of the kneser graph 
\begin_inset Formula $\omega\left(k\left(n,r\right)\right)=\left\lfloor \frac{n}{r}\right\rfloor $
\end_inset

, so 
\begin_inset Formula $\omega\left(P\right)=2$
\end_inset


\end_layout

\begin_layout Standard
A clique corresponds to a collection of disjoint sets.
\end_layout

\begin_layout Claim
\begin_inset CommandInset citation
LatexCommand cite
key "wolknes"

\end_inset

The coloring number 
\begin_inset Formula $\chi\left(k\left(n,r\right)\right)=n-2r+2$
\end_inset

, so 
\begin_inset Formula $\chi\left(P\right)=3$
\end_inset


\end_layout

\begin_layout Standard
This was a big open problem for many years.
 The optimal coloring is the following: Order the elements of the universe
 
\begin_inset Formula $u_{1},\dots,u_{n}$
\end_inset

, and divide them into 
\begin_inset Formula $3$
\end_inset

 pieces with sizes 
\begin_inset Formula $n-2r$
\end_inset

, 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

.
 Let 
\begin_inset Formula $x$
\end_inset

 be an 
\begin_inset Formula $r$
\end_inset

-set.
 If it intersects the first piece, color 
\begin_inset Formula $x$
\end_inset

 with the color 
\begin_inset Formula $i$
\end_inset

, where 
\begin_inset Formula $i=\min\left\{ i\mid u_{i}\in x\right\} $
\end_inset

.
 Otherwise, 
\begin_inset Formula $x$
\end_inset

 is contained entirely in the last two pieces.
 These remaining verticies form a subgraph, where each vertex has a unique
 neighbor, and these can be colored with two colors.
\end_layout

\begin_layout Claim
The independence number of the kneser graph is 
\begin_inset Formula $\alpha\left(k\left(n,r\right)\right)={n-1 \choose r-1}$
\end_inset

, so 
\begin_inset Formula $\alpha\left(P\right)=4$
\end_inset


\end_layout

\begin_layout Standard
The collection of 
\begin_inset Formula $r$
\end_inset

-subsets which each contain 
\begin_inset Formula $u_{1}$
\end_inset

 is a set of this size.
 It isn't hard to show this is optimal.
\end_layout

\begin_layout Claim
\begin_inset CommandInset citation
LatexCommand cite
key "wolknes"

\end_inset

The clique covering number is 
\begin_inset Formula $q\left(k\left(n,r\right)\right)=\left\lceil \frac{{n \choose k}}{\left\lfloor \frac{n}{k}\right\rfloor }\right\rceil $
\end_inset

, 
\begin_inset Formula $q\left(P\right)=5$
\end_inset

 
\end_layout

\begin_layout Claim
\begin_inset Formula $\vartheta\left(k\left(n,r\right)\right)={n-1 \choose r-1}$
\end_inset

, and 
\begin_inset Formula $\vartheta\left(k\left(n,r\right)\right)=\frac{n}{r}$
\end_inset

, so 
\begin_inset Formula $\vartheta\left(P\right)=4,\vartheta\left(\overline{P}\right)=\frac{5}{2}$
\end_inset


\end_layout

\begin_layout Standard
This is proven by Theorem 5 and some tricky algebra, but this avoids (or
 at least obscures) creating explicit orthonormal representations, which
 is the point of this example.
\end_layout

\begin_layout Section*
Relations between Graph Constants
\end_layout

\begin_layout Theorem
\begin_inset Formula $\alpha\left(G\right)\chi\left(G\right)\geq\mid V\left(G\right)\mid$
\end_inset


\end_layout

\begin_layout Standard
Each color is an independent set, and a proper coloring colors every vertex.
\end_layout

\begin_layout Theorem
For any graph 
\begin_inset Formula $G$
\end_inset

, 
\begin_inset Formula $\alpha\left(G\right)\leq\vartheta\left(G\right)\leq\chi\left(\overline{G}\right)=q\left(G\right)$
\end_inset


\end_layout

\begin_layout Standard
In an orthonormal representation, an independent set, 
\begin_inset Formula $\alpha$
\end_inset

, of 
\begin_inset Formula $G$
\end_inset

 must be sent to a collection of pairwise independent vectors.
 For such vectors, it is easy to see that 
\begin_inset Formula $\max_{v_{i}\in\alpha}\frac{1}{\left(\psi^{T}r\left(v_{i}\right)\right)^{2}}$
\end_inset

 is minimized when 
\begin_inset Formula $\psi=\frac{\sum_{v_{i}\in\alpha}r\left(v_{i}\right)}{\sqrt{|V\left(G\right)|}}$
\end_inset

 (when 
\begin_inset Formula $\psi$
\end_inset

 is between all the vectors.) In this case, 
\begin_inset Formula $\frac{1}{\left(\psi^{T}r\left(v_{i}\right)\right)^{2}}=\mid\alpha\mid$
\end_inset

, and this lower bound holds for all orthonormal representations.
 This shows 
\begin_inset Formula $\alpha\left(G\right)\leq\vartheta\left(G\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Suppose we have clique cover of size 
\begin_inset Formula $q\left(G\right)$
\end_inset

.
 Define an orthonormal representation by choosing 
\begin_inset Formula $q\left(G\right)$
\end_inset

 pairwise orthonormal vectors.
 Send each clique to one of these vectors.
 This provides an explicit orthonormal representation with valuation 
\begin_inset Formula $q\left(G\right)$
\end_inset

.
 The minimum over all orthonormal representations may be less.
\end_layout

\begin_layout Section*
Orthonormal Representations
\end_layout

\begin_layout Standard
We start with the graph 
\begin_inset Formula $\overline{k\left(n,r\right)}$
\end_inset

 and construct an optimal orthonormal representation in dimension 
\begin_inset Formula $n$
\end_inset

, with orthonormal basis 
\begin_inset Formula $u_{1},\dots,u_{n}$
\end_inset

 (overloading the names of the basis elements with the elements of the universe)
 The choice is obvious: disjoint sets need to go to orthonormal vectors.
 Set 
\begin_inset Formula $u_{i}^{T}r\left(v_{j}\right)=\frac{1}{\sqrt{r}}$
\end_inset

 if 
\begin_inset Formula $u_{i}\in v_{j}$
\end_inset

, and 
\begin_inset Formula $0$
\end_inset

 otherwise.
 Set 
\begin_inset Formula $\psi=\frac{1}{\sqrt{n}}\left(1,1,\dots,1\right)$
\end_inset

.
 It is immediate that this is an orthonormal representation with valuation
 
\begin_inset Formula $\frac{1}{\left(\psi^{T}r\left(v_{i}\right)\right)^{2}}=\frac{r\cdot n}{r^{2}}=\frac{n}{r}$
\end_inset

.
 This O.R.
 spans a space of dimension 
\begin_inset Formula $5$
\end_inset

.
\end_layout

\begin_layout Definition
Given an orthonormal representation, we can define the cost of a vertex
 to be 
\begin_inset Formula $c\left(v\right)=\left(\psi_{1}^{T}\left(r_{1}\left(v_{i}\right)\right)\right)^{2}$
\end_inset

.
 This corresponds to the quantum-mechanical probability of measuring 
\begin_inset Formula $r_{1}\left(v_{1}\right)$
\end_inset

 when measuring from state 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset citation
LatexCommand cite
key "knu93"

\end_inset

(Certification of Orthonormal Representations): if we have two orthonormal
 representations 
\begin_inset Formula $r_{1},r_{2}$
\end_inset

 of 
\begin_inset Formula $G$
\end_inset

 and 
\begin_inset Formula $\overline{G}$
\end_inset

 and for all 
\begin_inset Formula $i\in V\left(G\right)$
\end_inset

 we have 
\begin_inset Formula $c_{1}\left(v_{i}\right)=\frac{1}{\vartheta}$
\end_inset

, and we also have 
\begin_inset Formula $\sum_{i}c_{1}\left(v_{i}\right)c_{2}\left(v_{i}\right)=1$
\end_inset

, then 
\begin_inset Formula $\vartheta=\vartheta\left(G\right)$
\end_inset


\end_layout

\begin_layout Proof
We have the explicit orthonormal representation 
\begin_inset Formula $r_{1}$
\end_inset

, so 
\begin_inset Formula $\vartheta\left(G\right)\leq\vartheta$
\end_inset

.
 For the other direction, we use an alternate definition 
\begin_inset Formula $\vartheta$
\end_inset

, 
\begin_inset Formula $\vartheta\left(G\right)=\max_{Rep\left(\overline{G}\right)}\sum_{i}c\left(v_{i}\right).$
\end_inset


\begin_inset Formula 
\[
\min_{r\in O.R.\left(G\right)}\max_{i}\frac{1}{c_{r}\left(v_{i}\right)}=\vartheta\left(G\right)\leq\sum_{i}\vartheta c_{1}\left(v_{i}\right)c_{2}\left(v_{i}\right)=\sum_{i}c_{2}v_{i}\leq\max_{r\in O.R.\left(\overline{G}\right)}\sum_{i}c_{r}\left(v_{i}\right)=\vartheta\left(G\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The argument above also shows that certificates always exist.
\end_layout

\begin_layout Standard
The next definition is crucial, and describes the relationship between 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 and 
\begin_inset Formula $\vartheta\left(\overline{G}\right)$
\end_inset

.
 From the physical perspective, this will relate bell inequalities of completely
 different experiments.
 Can this relation be found using the Sheaf Theory?
\end_layout

\begin_layout Definition
\begin_inset CommandInset citation
LatexCommand cite
key "gro86"

\end_inset

Given a non-empty closed convex set 
\begin_inset Formula $P\subset\mathbb{R}_{+}^{n}$
\end_inset

 with the property that 
\begin_inset Formula $x\in P$
\end_inset

 and 
\begin_inset Formula $0\leq x^{\prime}\leq x$
\end_inset

 then 
\begin_inset Formula $x^{\prime}\in P$
\end_inset

, the antiblocker of 
\begin_inset Formula $P$
\end_inset

 is 
\begin_inset Formula 
\[
AB\left(P\right)=\left\{ x\in\mathbb{R}_{+}^{n}:y^{T}x\leq1\mbox{ for all }y\in P\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
The condition that 
\begin_inset Formula $0\leq x^{\prime}\leq x\implies x^{\prime}\in P$
\end_inset

 implies that 
\begin_inset Formula $AB\left(AB\left(P\right)\right)=P$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $P_{\vartheta}=\left\{ y\in\mathbb{R}_{+}^{n}\mid\sum_{i=1}^{n}y_{i}\leq\vartheta\right\} $
\end_inset

.
 Then 
\begin_inset Formula $AB\left(P\right)=C_{\frac{1}{\vartheta}}=\left\{ x\in\mathbb{R}_{+}^{n}\mid\forall i,x_{i}\leq\frac{1}{\vartheta}\right\} $
\end_inset

.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $y\in P_{\vartheta},x\in C_{\frac{1}{\vartheta}}$
\end_inset

.
 Then 
\begin_inset Formula $y^{T}x=\sum_{i=1}^{n}y_{i}x_{i}\leq\frac{1}{\vartheta}\sum_{i=1}^{n}y_{i}\leq1$
\end_inset

.
 This shows 
\begin_inset Formula $C_{\frac{1}{\vartheta}}\subset AB\left(P_{\vartheta}\right)$
\end_inset

.
 Conversely, if 
\begin_inset Formula $x\not\in C_{\frac{1}{\vartheta}}$
\end_inset

 for some 
\begin_inset Formula $i\in V\left(G\right)$
\end_inset

 
\begin_inset Formula $x_{i}>\frac{1}{\vartheta}$
\end_inset

.
 Choose 
\begin_inset Formula $y$
\end_inset

 such that 
\begin_inset Formula $y_{j}=0$
\end_inset

 when 
\begin_inset Formula $i\neq j$
\end_inset

, and 
\begin_inset Formula $y_{i}=\vartheta$
\end_inset

.
 Then 
\begin_inset Formula $y\in P_{\vartheta}$
\end_inset

, and 
\begin_inset Formula $x^{T}y>1$
\end_inset

, so 
\begin_inset Formula $x\notin AB\left(P_{\vartheta}\right)$
\end_inset


\end_layout

\begin_layout Definition
\begin_inset Formula $TH\left(G\right)=\left\{ \left(c\left(v_{i}\right),v_{i}\in V\left(G\right)\right)\in\mathbb{R}_{+}^{V\left(G\right)}\right\} $
\end_inset

.
 These are assignable probabilities, which (claim) satisfy the hypotheses
 of definition 6.
 (Note, these probablities do not need to sum to 1.
 We allow that some experiments have outcomes which are disregarded.
 The problem is intractable otherwise.)
\end_layout

\begin_layout Theorem
\begin_inset Formula $AB\left(TH\left(G\right)\right)=TH\left(\overline{G}\right)$
\end_inset


\end_layout

\begin_layout Standard
These concepts provide a geometric description of two definitions of 
\begin_inset Formula $\vartheta$
\end_inset

.
 
\begin_inset Formula $\vartheta$
\end_inset

 is the maximal of a linear functional over 
\begin_inset Formula $TH\left(G\right)$
\end_inset

: 
\begin_inset Formula $\vartheta\left(G\right)=\max_{\overline{O.R.}}\sum_{i=1}^{\left|V\left(G\right)\right|}c\left(v_{i}\right)$
\end_inset

.
 This linear functional has hyperplanes as its level sets, and the optimal
 value corresponds to a level set which lies tangent to 
\begin_inset Formula $TH\left(G\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 is the smallest simplex 
\begin_inset Formula $S_{\vartheta}$
\end_inset

 such that 
\begin_inset Formula $TH\left(G\right)\subset S_{\vartheta}$
\end_inset

.
 If we take the antiblocker of this picture, we seek the reciprocal of the
 largest cube 
\begin_inset Formula $C_{\frac{1}{\vartheta}}$
\end_inset

 such that 
\begin_inset Formula $C_{\frac{1}{\vartheta}}\subset AB\left(TH\left(G\right)\right)=TH\left(\overline{G}\right)$
\end_inset

.
 This explains the formula 
\begin_inset Formula $\vartheta\left(G\right)=\min_{O.R.}\max_{i}\frac{1}{c\left(v_{i}\right)}=\frac{1}{\max_{O.R.}\min_{i}c\left(v_{i}\right)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename antiblocker_def_theta.eps
	scale 50

\end_inset


\end_layout

\begin_layout Standard
Next, we give an orthonormal representation of 
\begin_inset Formula $\vartheta\left(P\right)$
\end_inset

, which will certify the optimality of the orthonormal representation given
 at the begining of this section.
\end_layout

\begin_layout Standard
Assume a basis of size 
\begin_inset Formula $10$
\end_inset

, 
\begin_inset Formula $\left\{ e_{s_{1}},e_{s_{2}},\dots,e_{s_{10}}\right\} $
\end_inset

 labeled by the 
\begin_inset Formula ${5 \choose 2}$
\end_inset

 subsets of the graph.
 Let 
\begin_inset Formula $\psi=\frac{1}{\sqrt{10}}\left(1,1,\dots,1\right)$
\end_inset

.
 Finally, assume that we will have 
\begin_inset Formula $e_{s_{i}}^{T}r\left(v_{j}\right)=x_{\mid s_{i}\cap v_{j}\mid}$
\end_inset

.
 This is a plausible assumption, because it will result in vectors whose
 orthongonality relations are invariant with respect to the automorphism
 group of 
\begin_inset Formula $P$
\end_inset

.
\end_layout

\begin_layout Standard
The fact that intersecting sets must be sent to orthonormal vectors translates
 into the contstraint
\begin_inset Formula 
\[
x_{0}^{2}+3x_{1}^{2}+4x_{0}x_{1}+2x_{1}x_{2}=0
\]

\end_inset


\end_layout

\begin_layout Standard
At the same time, we would like to minimize 
\begin_inset Formula $\frac{10\cdot\left(x_{2}^{2}+6x_{1}^{2}+3x_{0}^{2}\right)}{\left(x_{2}+6x_{1}+3x_{0}\right)^{2}}$
\end_inset

.
 According to Wolfram Alpha the minimum is 
\begin_inset Formula $4$
\end_inset

, when 
\begin_inset Formula $\left(x_{0},x_{1},x_{2}\right)=\left(1,-4-\sqrt{15},6+\sqrt{15}\right)$
\end_inset

, or when 
\begin_inset Formula $\left(x_{0},x_{1},x_{2}\right)=\left(a,b,c\right)=\left(1,\sqrt{15}-4,6-\sqrt{15}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left(\begin{array}{cccccccccc}
c & b & b & b & b & b & b & a & a & a\\
b & c & b & b & b & a & a & b & b & a\\
b & b & c & b & a & b & a & b & a & b\\
b & b & b & c & a & a & b & a & b & b\\
b & b & a & a & c & b & b & b & b & a\\
b & a & b & a & b & c & b & b & a & b\\
b & a & a & b & b & b & c & a & b & b\\
a & b & b & a & b & b & a & c & b & b\\
a & b & a & b & b & a & b & b & c & b\\
a & a & b & b & a & b & b & b & b & c
\end{array}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
These vectors span a space of dimension 
\begin_inset Formula $6$
\end_inset

.
\end_layout

\begin_layout Remark
For any given graph 
\begin_inset Formula $G$
\end_inset

, it is not true that all optimal orthogonal representations have the same
 dimension.
 For example, there are two optimal orthonormal representations of 
\begin_inset Formula $C_{4}$
\end_inset

: 
\begin_inset Formula $\left\{ \left(0,1\right),\left(0,1\right),\left(1,0\right),\left(1,0\right)\right\} $
\end_inset

 with handle 
\begin_inset Formula $\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)$
\end_inset

, and 
\begin_inset Formula $\left\{ \left(a,a,0\right),\left(a,-a,0\right),\left(a,0,a\right),\left(a,0,-a\right)\right\} $
\end_inset

 with handle 
\begin_inset Formula $\left(1,0,0\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Finally, we apply the certification theorem.
 For the O.R.
 above, each cost is 
\begin_inset Formula $\frac{1}{4}$
\end_inset

, and 
\begin_inset Formula $\sum_{i=1}^{10}\frac{1}{4}\cdot\frac{2}{5}=1$
\end_inset

.
 Hence, the O.R.
 above is optimal.
 Similarly, our 
\begin_inset Formula $O.R.$
\end_inset

 of 
\begin_inset Formula $\overline{K\left(n,r\right)}$
\end_inset

 can be seen to be optimal.
\end_layout

\begin_layout Section*
Vector Colorings of Graphs
\end_layout

\begin_layout Definition
Given a graph 
\begin_inset Formula $G$
\end_inset

, we assign a unit vector to each vertex.
 This time, we would like adjacent verticies to be sent to vectors whose
 dot product is as negative as possible.
 If 
\begin_inset Formula $\chi\left(G\right)=k$
\end_inset

, then we can associate each color with a vector in the regular 
\begin_inset Formula $k$
\end_inset

-simplex in 
\begin_inset Formula $\mathbb{R}^{k+1}$
\end_inset

.
 Such vectors have inner product 
\begin_inset Formula $\frac{-1}{k-1}$
\end_inset

.
 In light of this, we define 
\begin_inset Formula $\chi_{vec}\left(G\right)=\min\left\{ k\mid v_{i}^{T}v_{j}=\frac{-1}{k-1}\right\} $
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset Formula $\chi_{vec}\left(G\right)=\vartheta\left(\overline{G}\right)$
\end_inset


\end_layout

\begin_layout Standard
This is proven by the fact that the two problems can be expressed as semidefinit
e-programming duals of one another.
 (The duality is between 
\begin_inset Formula $\chi_{vec}$
\end_inset

 and 
\begin_inset Formula $\max_{\overline{O.R.}}\sum\left(\psi^{T}v_{i}\right)^{2}$
\end_inset

.) Alternatively, there is a concrete way to move between optimal representations
 of the coloring problem and optimal representations for 
\begin_inset Formula $\vartheta$
\end_inset

.
\end_layout

\begin_layout Proposition
A vector coloring 
\begin_inset Formula $x_{i}$
\end_inset

 is optimal (having value 
\begin_inset Formula $\vartheta$
\end_inset

) iff 
\begin_inset Formula $u_{i}=\frac{1}{\sqrt{\vartheta}}\left(\psi+\sqrt{\vartheta-1}x_{i}\right)$
\end_inset

 is an optimal Orthonormal representation (also having value 
\begin_inset Formula $\vartheta$
\end_inset

).
\end_layout

\begin_layout Proof
If 
\begin_inset Formula $x_{i}$
\end_inset

 is an optimal vector-coloring for 
\begin_inset Formula $G$
\end_inset

 with coloring number 
\begin_inset Formula $\vartheta$
\end_inset

, and 
\begin_inset Formula $\psi$
\end_inset

 is some unit vector orthogonal to each 
\begin_inset Formula $x_{i}$
\end_inset

, then we obtain an orthonormal representation 
\begin_inset Formula $u_{i}=\frac{1}{\sqrt{\vartheta}}\left(\psi+\sqrt{\vartheta-1}x_{i}\right)$
\end_inset

.
 First, observe that these are all unit vectors.
 Secondly, let 
\begin_inset Formula $u_{i}\neq u_{j}$
\end_inset

 correspond to non-adjacent verticies in 
\begin_inset Formula $\overline{G}$
\end_inset

, so that 
\begin_inset Formula $x_{i}^{T}x_{j}=\frac{-1}{\vartheta-1}$
\end_inset

.
 Then 
\begin_inset Formula $u_{i}^{T}u_{j}=\frac{1}{\vartheta}\left(1+\left(\vartheta-1\right)\frac{-1}{\vartheta-1}\right)=0$
\end_inset

, so 
\begin_inset Formula $u$
\end_inset

 is an orthogonal representation of 
\begin_inset Formula $\overline{G}$
\end_inset

.
 Also, we have 
\begin_inset Formula $\frac{1}{\left(\psi^{T}u_{i}\right)^{2}}=\vartheta$
\end_inset

.
\end_layout

\begin_layout Proof
Conversely, if we start with the orthonormal representation with value 
\begin_inset Formula $\vartheta$
\end_inset

 (so 
\begin_inset Formula $\vartheta=\frac{1}{\left(u_{i}^{T}\psi\right)^{2}}$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 We have not yet shown that it's always possible to achieve equality, but
 it can be seen from the antiblocker picture.) we can recover the coloring
 by 
\begin_inset Formula $x_{i}=\frac{\sqrt{\vartheta}u_{i}-\psi}{\sqrt{\vartheta-1}}$
\end_inset

.
 Now, if 
\begin_inset Formula $x_{i}\sim x_{j}$
\end_inset

 in 
\begin_inset Formula $G$
\end_inset

, then 
\begin_inset Formula $x_{i}^{T}x_{j}=\frac{-\sqrt{\vartheta}u_{i}^{T}\psi-\sqrt{\vartheta}u_{j}^{T}\psi+1}{\vartheta-1}=\frac{-1}{\vartheta-1}$
\end_inset

 so the coloring has value 
\begin_inset Formula $\vartheta$
\end_inset

.
 Also, 
\begin_inset Formula $x_{i}^{2}=\frac{\left(\vartheta+1\right)-2\sqrt{\vartheta}u_{i}\cdot\psi}{\vartheta-1}=1$
\end_inset

.
 (There is a slight issue: if 
\begin_inset Formula $u_{i}\cdot\psi=-\sqrt{\vartheta}$
\end_inset

, we need to reassign 
\begin_inset Formula $u_{i}\mapsto-u_{i}$
\end_inset

.)
\end_layout

\begin_layout Proof
\begin_inset Graphics
	filename coloring_representation.eps
	scale 60

\end_inset


\end_layout

\begin_layout Standard
Next, we will provide an optimal vector coloring of 
\begin_inset Formula $P$
\end_inset

.
 Assume a basis of size 
\begin_inset Formula $5$
\end_inset

, and that we will map 
\begin_inset Formula $\star\star\circ\circ\circ\mapsto\left(a,a,b,b,b\right)$
\end_inset

, and extend this map by permutations of 
\begin_inset Formula $S_{5}$
\end_inset

.
 If 
\begin_inset Formula $x,y$
\end_inset

 are two vector representations of intersecting sets, we would like to minimize
\begin_inset Formula 
\[
min_{a,b}\frac{x^{T}y}{\left\Vert x\right\Vert \left\Vert y\right\Vert }=\frac{4ab+b^{2}}{2a^{2}+3b^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
The minumum occurs at 
\begin_inset Formula $a=-3,b=2$
\end_inset

, and gives 
\begin_inset Formula 
\[
\frac{x^{T}y}{\left\Vert x\right\Vert \left\Vert y\right\Vert }=\frac{-24+4}{18+12}=-\frac{2}{3}=\frac{-1}{\frac{5}{2}-1}
\]

\end_inset


\end_layout

\begin_layout Standard
Using numpy.linalg, we can find that these vectors span a space of dimension
 
\begin_inset Formula $4$
\end_inset

.
\end_layout

\begin_layout Standard
Finally, an optimal vector coloring of 
\begin_inset Formula $\overline{P}$
\end_inset

 can be found by assuming a basis of size 
\begin_inset Formula $10$
\end_inset

 (the same basis we used for 
\begin_inset Formula $\vartheta\left(P\right)$
\end_inset

) and three variables, 
\begin_inset Formula $x_{0},x_{1},x_{2}$
\end_inset

.
 This gives us the optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min_{x_{0},x_{1},x_{2}}\frac{x^{T}y}{\left\Vert x\right\Vert \left\Vert y\right\Vert }=\frac{x_{0}^{2}+3x_{1}^{2}+4x_{0}x_{1}+2x_{1}x_{2}}{3x_{0}^{2}+6x_{1}^{2}+x_{2}^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
The minimum (according to Wolfram) is found at 
\begin_inset Formula $\left(-\frac{1}{\sqrt{18}},\frac{1}{\sqrt{18}},-\frac{1}{\sqrt{2}}\right)$
\end_inset

 and gives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{x^{T}y}{\left\Vert x\right\Vert \left\Vert y\right\Vert }=\frac{\frac{1}{18}+\frac{3}{18}-4\frac{1}{18}-2\frac{1}{6}}{3\frac{1}{18}+6\frac{1}{18}+\frac{1}{2}}=\frac{-6}{18}=\frac{-1}{4-1}
\]

\end_inset


\end_layout

\begin_layout Standard
This spans a space of dimension 
\begin_inset Formula $5$
\end_inset

.
\end_layout

\begin_layout Problem
Since we have established that vector colorings correspond to orthonormal
 representations, we actually have 
\begin_inset Formula $2$
\end_inset

 orthonormal representations of the Petersen Graph and 
\begin_inset Formula $2$
\end_inset

 for its complement.
 Are these the same?
\end_layout

\begin_layout Section*
Quantum Measurements
\end_layout

\begin_layout Standard
We fix a finite-dimensional vector space, known as the state space.
 Pure quantum states will be given by unit vectors in this space.
\end_layout

\begin_layout Quote
A projective measurement is described by an observable, 
\begin_inset Formula $M$
\end_inset

, a Hermitian operator on the state space of the system being observed.
 The observable has a spectral decomposition 
\begin_inset Formula 
\[
M=\sum_{m}mP_{m}
\]

\end_inset


\end_layout

\begin_layout Quote
where 
\begin_inset Formula $P_{m}$
\end_inset

 is the projector onto the eigenspace of 
\begin_inset Formula $M$
\end_inset

 with eigenvalue 
\begin_inset Formula $m$
\end_inset

.
 The possible outcomes of the measurement correspond to the eigenvalues,
 
\begin_inset Formula $m$
\end_inset

, of the observable.
 Upon measuring the state 
\begin_inset Formula $\left|\psi\right\rangle $
\end_inset

, the probability of getting result 
\begin_inset Formula $m$
\end_inset

 is given by
\begin_inset Formula 
\[
\left\langle \psi\right|P_{m}\left|\psi\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Quote
Given that the outcome 
\begin_inset Formula $m$
\end_inset

 occured, the state of the quantum system immediately after the measurement
 is 
\begin_inset Formula $\frac{P_{m}\left|\psi\right\rangle }{\sqrt{p\left(m\right)}}$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "nie10"

\end_inset


\end_layout

\begin_layout Standard
(!) Naimark's Dilation Theorem states that all quantum measurements can
 be viewed as projective measurements on a larger system.
 The Spectral Theorem states that a Hermitian operator 
\begin_inset Formula $M$
\end_inset

 always has a decomposition of the form 
\begin_inset Formula 
\[
M=\sum_{m}mP_{m}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $m$
\end_inset

 ranges over real numbers, and the ranges of the 
\begin_inset Formula $P_{m}$
\end_inset

's are pairwise orthogonal.
 Conversely, if we start with a collection of pairwise orthogonal vectors
 
\begin_inset Formula $\left\{ \left|m\right\rangle \right\} _{m=1}^{n}$
\end_inset

which span the state space, we can create a Hermitian operator which has
 those vectors as its eigenvectors: 
\begin_inset Formula $M=\sum_{m=1}^{n}m\left|m\right\rangle \left\langle m\right|$
\end_inset

.
 Thus, when specifying a PVM, we only need to supply an Orthonormal Basis.
\end_layout

\begin_layout Example
Polarizing sunglasses are an example of a PVM.
\end_layout

\begin_layout Example
Light consists of an electric wave inducing a magnetic wave and vice versa.
 The direction of the electric wave determines the polarization of the light.
 A photon can be polarized in any 
\begin_inset Formula $2$
\end_inset

-dimensional direction.
 Polarized lenses in sunglasses will let a photon through if it is polarized
 vertically, and will block it if it is polarized horizontally.
 The photon hitting the lens is a measurement.
 The state space is 
\begin_inset Formula $2$
\end_inset

-dimensional, corresponding to the polarization directions.
 We are lucky in this case that polarization between horizontal and vertical
 directions has a direct physical meaning- that the light is polarized in
 a diagonal direction.
 Usually, this is not the case.
 Let 
\begin_inset Formula $\left|v\right\rangle $
\end_inset

 be the state of light which is polarized in the vertical direction and
 
\begin_inset Formula $\left|h\right\rangle $
\end_inset

be the state of light polarized in the horizontal direction.
 When a vertically polarized photon hits a vertically polarized lens, it
 will surely pass through.
 We represent the observable for this lens by 
\begin_inset Formula $L=1\cdot\left|v\right\rangle \left\langle v\right|+0\cdot\left|h\right\rangle \left\langle h\right|$
\end_inset

.
 Suppose we have light that is polarized diagonally, 
\begin_inset Formula 
\[
\left|\psi\right\rangle =\frac{1}{\sqrt{2}}\left|v\right\rangle +\frac{1}{\sqrt{2}}\left|h\right\rangle 
\]

\end_inset

The probability of the light passing through the lense is 
\begin_inset Formula $\left(\left\langle v\right|+\left\langle h\right|\right)\left|v\right\rangle \left\langle v\right|\left(\left|v\right\rangle +\left|h\right\rangle \right)=\frac{1}{2}$
\end_inset

, and if it does so, its state is 
\begin_inset Formula $\frac{\left|v\right\rangle \left\langle v\right|\frac{1}{\sqrt{2}}\left(\left|v\right\rangle +\left|h\right\rangle \right)}{\sqrt{\frac{1}{2}}}=\left|v\right\rangle $
\end_inset

.
 Observe that the probability that the light passes through the lens is
 the square of the inner product and the resultant state we're interested
 in.
 This is true in general.
\end_layout

\begin_layout Section*
Forward Example: The CHSH Inequality
\end_layout

\begin_layout Standard
There are two conceivable ways to use the machinery of 
\begin_inset Formula $\vartheta$
\end_inset

 to investigate contextuality.
 Usually we start with a collection of measurements and a linear functional
 on the outcomes of these measurements.
 By drawing the exclusion graph of the measurements, we can identify bounds
 (Bell Inequalities) on these linear functionals.
 We will see that these bounds are given by 
\begin_inset Formula $\vartheta$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

.
 Alternatively, we can go backwards and search for graphs which have a large
 gap between 
\begin_inset Formula $\vartheta$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

, then find orthonormal representations to realize these graphs as collections
 of quantum experiments.
 Ultimately, this is the direction that I would like to pursue, but we should
 start with the standard technique
\begin_inset CommandInset citation
LatexCommand cite
key "cab14"

\end_inset

 first.
\end_layout

\begin_layout Standard
As an example of the forward method, we will investigate the CHSH inequality.
 Imagine Alice and Bob, separated in time and space and unable to communicate.
 However, they share qubits which make up a quantum state.
 That is, these particles may be entangled, so the total state space is
 
\begin_inset Formula $4$
\end_inset

-dimensional.
 Alice may choose between two measurements 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $A^{\prime}$
\end_inset

, and Bob may also choose between two measurements 
\begin_inset Formula $B$
\end_inset

 and 
\begin_inset Formula $B^{\prime}$
\end_inset

, so there are 
\begin_inset Formula $4$
\end_inset

 total measurements which may occur: 
\begin_inset Formula $\left\{ AB,A^{\prime}B,AB^{\prime},A^{\prime}B^{\prime}\right\} $
\end_inset

.
 Each of Alice and Bob's local measurements yield an outcome of 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

, so each total measurement has 
\begin_inset Formula $4$
\end_inset

 possible outcomes.
\end_layout

\begin_layout Standard
(One formulation of) the CHSH inequality begins with such a scenario and,
 if Alice and Bob both choose their experiments uniformly at random, and
 assigns a value of 
\begin_inset Formula $1$
\end_inset

 whenever the their outcomes agree and 
\begin_inset Formula $A,B$
\end_inset

 is not chosen, and a value of 
\begin_inset Formula $-1$
\end_inset

 whenever their outcomes disagree and 
\begin_inset Formula $A,B$
\end_inset

 is not chosen.
 If 
\begin_inset Formula $A,B$
\end_inset

 is chosen, then we reverse the valuations, so that 
\begin_inset Formula $1$
\end_inset

 is assigned when their measurements disagree and 
\begin_inset Formula $-1$
\end_inset

 when the agree.
 The CHSH inequality concerns the expectation of such a valuation.
 To formalize this, let 
\begin_inset Formula $E\left(A^{(\prime)},B^{(\prime)}\right)$
\end_inset

 be the probability that the outcomes agree, given that 
\begin_inset Formula $A^{(\prime)},B^{(\prime)}$
\end_inset

 is measured.
 Then we have a value 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S=-E\left(A,B\right)+E\left(A^{\prime},B\right)+E\left(A,B^{\prime}\right)+E\left(A^{\prime},B^{\prime}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The CHSH inequality states that under classical assumptions 
\begin_inset Formula 
\[
S\leq2
\]

\end_inset


\end_layout

\begin_layout Standard
We will see where this bound comes from.
 Yet using quantum mechanics, we may achive a value of 
\begin_inset Formula 
\[
S=2\sqrt{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Concretely, the measurements which realize this bound are: Alice may either
 measure in the standard basis (
\begin_inset Formula $A$
\end_inset

), or in the basis 
\begin_inset Formula $\left\{ \frac{\left|0\right\rangle +\left|1\right\rangle }{\sqrt{2}},\frac{\left|0\right\rangle -\left|1\right\rangle }{\sqrt{2}}\right\} $
\end_inset

 (
\begin_inset Formula $A^{\prime}$
\end_inset

).
 Bob may either measure in the basis 
\begin_inset Formula $\left\{ \frac{\cos\left(\frac{\pi}{8}\right)\left|0\right\rangle -\sin\left(\frac{\pi}{8}\right)\left|1\right\rangle }{\sqrt{2}},\frac{\sin\left(\frac{\pi}{8}\right)\left|0\right\rangle +\cos\left(\frac{\pi}{8}\right)\left|1\right\rangle }{\sqrt{2}}\right\} $
\end_inset

(
\begin_inset Formula $B$
\end_inset

) and in the basis 
\begin_inset Formula $\left\{ \frac{\cos\left(\frac{\pi}{8}\right)\left|0\right\rangle +\sin\left(\frac{\pi}{8}\right)\left|1\right\rangle }{\sqrt{2}},\frac{-\sin\left(\frac{\pi}{8}\right)\left|0\right\rangle +\cos\left(\frac{\pi}{8}\right)\left|1\right\rangle }{\sqrt{2}}\right\} $
\end_inset

 (
\begin_inset Formula $B^{\prime}$
\end_inset

).
 (!) (TODO: check that this is accurate.)
\end_layout

\begin_layout Standard
To derive these bounds we first draw the exclusion graph for this scenario.
 The verticies of the graph are the 
\begin_inset Formula $16$
\end_inset

 total outcomes which may occur (or, if you like, pairs of measurements
 and outcomes).
 Edges are drawn between vertices which cannot cooccur.
 For example, there is an edge between 
\begin_inset Formula $\left(0,0\mid AB\right)$
\end_inset

 and 
\begin_inset Formula $\left(1,0\mid AB^{\prime}\right)$
\end_inset

 because it is impossible that the local measurement 
\begin_inset Formula $A$
\end_inset

 gives both 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename CHSHgraph.eps
	scale 25

\end_inset


\end_layout

\begin_layout Standard
Once the exclusion graph, 
\begin_inset Formula $H$
\end_inset

, is drawn, we obtain a weighting on the vertices which is determined by
 our valuation.
 For example, whenever the outcome 
\begin_inset Formula $\left(1,1\mid AB^{\prime}\right)$
\end_inset

 occurs, we add 
\begin_inset Formula $1$
\end_inset

 to our counter, and whenever 
\begin_inset Formula $\left(0,0\mid AB\right)$
\end_inset

 occurs, we subtract 
\begin_inset Formula $1$
\end_inset

 from our counter.
 Thus, each vertex gets a weight 
\begin_inset Formula $w:V\left(G\right)\to\mathbb{R}$
\end_inset

.
 We can insist that these weights be positive by adding 
\begin_inset Formula $1$
\end_inset

 to all the experiments so that our final weighting corresponds to giving
 
\begin_inset Formula $w\left(v\right)=2$
\end_inset

 for the black vertices and 
\begin_inset Formula $w\left(v\right)=0$
\end_inset

 for the white vertices.
 Typically, and in this case, all of the weights will be the same, except
 for those which are 
\begin_inset Formula $0$
\end_inset

.
 Our weighting therefore identifies a subgraph, 
\begin_inset Formula $H^{\prime}$
\end_inset


\end_layout

\begin_layout Standard
To derive the classical bounds, observe that any classical state of the
 system must assign outcomes to all measurements, even those which have
 not occured.
 This means that a classical state will correspond to an independent set
 in the graph.
 It is intuitive (and easy to show, since 
\begin_inset Formula $S$
\end_inset

 is a linear functional) 
\begin_inset Formula $S$
\end_inset

 is maximized at a particular classical state rather than a mixture of them.
 The maximum value for 
\begin_inset Formula $S$
\end_inset

, classically, will correspond to an independent set and this independent
 set will restrict to an independent set of 
\begin_inset Formula $H^{\prime}$
\end_inset

.
 Since 
\begin_inset Formula $\alpha\left(H^{\prime}\right)=3$
\end_inset

, we can recover the classical bound on 
\begin_inset Formula $S$
\end_inset

 by undoing our manipulations to the weighting.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S\leq_{classical}2\cdot3-4=2
\]

\end_inset


\end_layout

\begin_layout Standard
With respect to quantum mechanics, each of our outcomes corresponds to an
 eigenvector of an observable.
 Thus, each vertex in 
\begin_inset Formula $H^{\prime}$
\end_inset

 receives a vector.
 If the two outcomes associated with these vectors are exclusive, then the
 vectors must be orthogonal.
 Otherwise, it would be possible to measure the system, obtain one outcome,
 then measure the system again and obtain the other, exclusive outcome.
\end_layout

\begin_layout Standard
Therefore, the eigenvectors of the observables form an orthogonal representation
 of 
\begin_inset Formula $\overline{H^{\prime}}$
\end_inset

.
 Recall our dual definition for 
\begin_inset Formula $\vartheta\left(G\right)=\max_{\overline{OR}}\sum_{i}c\left(v_{i}\right)$
\end_inset

 is exactly the maximum expected value in the quantum setting.
 Thus, 
\begin_inset Formula 
\[
S\leq2\cdot\vartheta\left(G\right)-4=2\cdot\left(2+\sqrt{2}\right)-4=2\sqrt{2}
\]

\end_inset


\end_layout

\begin_layout Standard
In this case, the bound is tight.
 This is not always the case, because the dimension of the optimal orthonormal
 representation may be too large.
\end_layout

\begin_layout Section*
From Orthonormal Representations to Quantum Circuits
\end_layout

\begin_layout Standard
Next, we would like to sketch how we might find collections of quantum circuits
 which exhibit large amounts of contextuality.
 This procedure comes with many unanswered questions.
\end_layout

\begin_layout Enumerate
Choose an optimal coloring of the graph.
\end_layout

\begin_layout Enumerate
Produce an optimal orthonormal representation for this graph.
\end_layout

\begin_layout Enumerate
Each color corresponds to a collection of pairwise orthonormal vectors.
 Extend each collection to an orthonormal basis.
\end_layout

\begin_layout Enumerate
Find the unitary transformations from the given bases to the standard (computati
onal) basis.
\end_layout

\begin_layout Enumerate
All unitary transformations can be implemented by quantum gates.
\end_layout

\begin_layout Enumerate
Our circuits consist of these unitary transformations, followed by measurement
 in the computational basis.
\end_layout

\begin_layout Standard
For a quantum circuit which takes some input state, 
\begin_inset Formula $\psi$
\end_inset

, applies a unitary transformation, 
\begin_inset Formula $U$
\end_inset

, then measures in the computational basis, the possible outcomes will be
 the elements of the computational basis.
 If 
\begin_inset Formula $x$
\end_inset

 is one such basis element, the probability of measuring 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula $\left\langle U\psi,x\right\rangle ^{2}=\left\langle \psi,U^{\star}x\right\rangle ^{2}$
\end_inset

.
 It is easier to think of the unitary transformations as moving the computationa
l basis than moving the state.
 We will think of these quantum circuits as collections of bases under which
 to measure a particular state.
\end_layout

\begin_layout Standard
Since any two orthogonal vectors can be extended to a basis and we can perform
 a measurement (which will reveal one outcome) in that basis, two orthogonal
 vectors represent incompatible outcomes.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename colored_Kneser.eps
	scale 50

\end_inset


\end_layout

\begin_layout Standard
If we perform these steps, we will arive at 
\begin_inset Formula $\chi\left(G\right)$
\end_inset

 quantum circuits whose statistics on the input state 
\begin_inset Formula $\psi$
\end_inset

 cannot be explained classically.
 Specifically, the results corresponding to vectors in our original Orthonormal
 Representation (prior to extending to a basis) will occur more often than
 is possible classically.
 The dimension of the orthonormal representation, 
\begin_inset Formula $d$
\end_inset

, is exactly the dimension of the hilbert space in which the quantum circuit
 lives, so we will need 
\begin_inset Formula $\left\lceil \log_{2}d\right\rceil $
\end_inset

 qubits to implement such a circuit.
\end_layout

\begin_layout Standard
Some questions must be raised:
\end_layout

\begin_layout Enumerate
Does the initial coloring matter?
\end_layout

\begin_layout Enumerate
Can we effectively find the O.R.
 in step 2?
\end_layout

\begin_layout Enumerate
How can we control the dimension of the optimal orthogonal representation?
 Can we use a non-optimal representation in a smaller dimension? (I have
 a specific construction, due to Lovasz in mind.)
\end_layout

\begin_layout Enumerate
How can we control the circuit complexity of our resultant unitaries?
\end_layout

\begin_layout Enumerate
Does it matter how extend our colored sets to bases?
\end_layout

\begin_layout Enumerate
Can any of these circuits be achieved using only Clifford gates? Can we
 find a result linking the contextual resources (such as magic states) used
 to form the unitaries, and the final contextuality?
\end_layout

\begin_layout Section*
Graph Theory and Optimization
\end_layout

\begin_layout Standard
If we have an exclusivity graph 
\begin_inset Formula $G$
\end_inset

, we can consider the probabilities which may be assigned to vectors 
\begin_inset Formula $\mathbb{R}_{+}^{V\left(G\right)}$
\end_inset

.
 This identification also maps subgraphs of 
\begin_inset Formula $G$
\end_inset

 to their incidence vectors.
 Recall that independent sets of our graph correspond to the classical,
 deterministic probablility distribution.
 The collection of classical distributions is therefore described by convex
 sums of our independent sets.
 The set of classical states is given by the 
\begin_inset Quotes eld
\end_inset

vertex polytope,
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $VP$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
VP\coloneqq CvxHull(\left\{ vec_{\alpha}\mid\alpha\mbox{ is independent}\right\} )
\]

\end_inset


\end_layout

\begin_layout Standard
We may express 
\begin_inset Formula $\alpha$
\end_inset

 (we will omit the argument 
\begin_inset Formula $G$
\end_inset

 in the notation) as an optimization problem over this set.
 Specifically, 
\begin_inset Formula $\alpha$
\end_inset

 may be defined by an integer linear program.
 We begin with variables 
\begin_inset Formula $\left\{ x_{i}\in\left\{ 0,1\right\} \right\} _{i=1}^{V\left(G\right)}$
\end_inset

 which are intepreted as: 
\begin_inset Formula $x_{i}=1$
\end_inset

 when 
\begin_inset Formula $v_{i}$
\end_inset

 is in the indendent set in question.
 Our program is then: 
\begin_inset Formula 
\[
\alpha=\max_{x}\left(\sum_{i=1}^{V\left(G\right)}x_{i}\right)
\]

\end_inset

such that 
\begin_inset Formula $x_{i}\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula 
\[
x_{i}+x_{j}\leq1,\mbox{ for }v_{i}\sim v_{j}
\]

\end_inset


\end_layout

\begin_layout Standard
Equivalently, we could write our constraint as 
\begin_inset Formula $x\in VP$
\end_inset

.
\end_layout

\begin_layout Standard
This problem is intractible, so we might formulate a linear program by allowing
 
\begin_inset Formula $x_{i}\in\left[0,1\right]$
\end_inset

 to take on a continum of values.
 The resulting problem is tractible, but is too relaxed.
 For example, 
\begin_inset Formula $\left(\frac{1}{2},\frac{1}{2},\frac{1}{2}\right)$
\end_inset

 is a solution to the triangle.
 Thus, we choose a different generalization of the constraints:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha^{\star}=\max_{x}\left(\sum_{i=1}^{V\left(G\right)}x_{i}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
such that 
\begin_inset Formula $x_{i}\in\left[0,1\right]$
\end_inset

 and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{i\in K}x_{i}\leq1\mbox{ for all cliques \ensuremath{K}}
\]

\end_inset


\end_layout

\begin_layout Standard
In this problem, our feasible set is the 
\begin_inset Quotes eld
\end_inset

fractional vertex packing polytope
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
FVP=\left\{ x\mid\sum_{i\in K}x_{i}\leq1,\mbox{\forall\ cliques }K\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
This program is important enough to get a name: 
\begin_inset Formula $\alpha^{\star}$
\end_inset

, the fractional packing number.
 Moreover, the 
\begin_inset Formula $\alpha^{\star}\neq\alpha$
\end_inset

, since, for the 
\begin_inset Formula $5$
\end_inset

-cycle we have the solution 
\begin_inset Formula $\left(\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The fractional packing number has a nice interpretation: If our variables
 
\begin_inset Formula $x_{i}$
\end_inset

 correspond to probabilities, then our constraint enforces that the sum
 of probabilities of pairwise exclusive events be bounded by 
\begin_inset Formula $1$
\end_inset

, which is implied by one of Kolmogorov's Axioms for probability.
 General probabilistic models have their Bell Inequalities bounded by 
\begin_inset Formula $\alpha^{\star}$
\end_inset

 rather than 
\begin_inset Formula $\vartheta$
\end_inset

.
\end_layout

\begin_layout Standard
This problem is 
\begin_inset Formula $NP$
\end_inset

-hard because there may be exponentially many cliques.
 For perfect graphs, however, 
\begin_inset Formula $\alpha=\alpha^{\star}$
\end_inset

 and both are polynomial-time computable.
\end_layout

\begin_layout Theorem
\begin_inset Formula $FVP\left(\overline{G}\right)=AB\left(VP\left(G\right)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $x\in FVP\left(\overline{G}\right)$
\end_inset

.
 If we show that 
\begin_inset Formula $x^{T}v_{\alpha}\leq1$
\end_inset

, for all indepdent sets 
\begin_inset Formula $\alpha$
\end_inset

, we will have shown that 
\begin_inset Formula $FVP\left(\overline{G}\right)\subset AB\left(VP\left(G\right)\right)$
\end_inset

.
 Since 
\begin_inset Formula $\alpha$
\end_inset

 is an independent set of 
\begin_inset Formula $G$
\end_inset

, it is also a clique of 
\begin_inset Formula $\overline{G}$
\end_inset

, so 
\begin_inset Formula $\sum_{a\in\alpha}x_{a}+\sum_{b\in G-\alpha}x_{b}\cdot0\leq1+0=1$
\end_inset

.
 The argument works conversely as well: If 
\begin_inset Formula $x^{T}y\leq1$
\end_inset

 for all 
\begin_inset Formula $y\in VP\left(G\right)$
\end_inset

, then 
\begin_inset Formula $x^{T}v_{\alpha}\leq1$
\end_inset

 for all independent 
\begin_inset Formula $\alpha$
\end_inset

.
 Again, since these independent 
\begin_inset Formula $\alpha$
\end_inset

 in 
\begin_inset Formula $G$
\end_inset

 become cliques 
\begin_inset Formula $\omega$
\end_inset

 in 
\begin_inset Formula $\overline{G}$
\end_inset

, our constraints precisely name 
\begin_inset Formula $FVP$
\end_inset

.
\end_layout

\begin_layout Section*
Linear Programming Duality
\end_layout

\begin_layout Standard
Every linear program comes with a dual program with the same optimum.
 The linear program
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mbox{Max}_{x}\left\{ c^{T}x\mid Ax\leq b,x\geq0\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
comes with the dual
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mbox{Min}_{y}\left\{ b^{T}y\mid A^{T}y\geq c,y\geq0\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
If the primal problem asks: 
\begin_inset Quotes eld
\end_inset

Given my ingredients and recipies for baking various cakes and the profits
 for each type of cake, how much of each cake should I make in order to
 maximize my profit? And what is my final profit?
\begin_inset Quotes erd
\end_inset

 Then the dual problem asks 
\begin_inset Quotes eld
\end_inset

What is the minimum amount of money I would accept to sell my ingredients,
 given that I could use them to make cakes and later sell these cakes? And
 how much money would I make from doing this.
\begin_inset Quotes erd
\end_inset

 In this formulation, it is intuitive that the primal and dual problems
 will have the same optimal.
\end_layout

\begin_layout Standard
Another way to think about the dual program that it provides upper bounds
 for the primal problem.
 In this perspective, the new variables 
\begin_inset Formula $y$
\end_inset

 refer to different ways to linearly combine our constraint equation to
 achive(on the left side) a linear functionals 
\begin_inset Formula $A^{T}y$
\end_inset

 which is an upper bound on our primal functional, 
\begin_inset Formula $c$
\end_inset

.
 Meanwhile, on the right side of the inequality, we get 
\begin_inset Formula $b^{T}y$
\end_inset

, so this is an upper bound
\begin_inset CommandInset citation
LatexCommand cite
key "tal13"

\end_inset

.
\end_layout

\begin_layout Standard
A third way to think about the dual problem is via Lagrange Multipliers.
 When optimizing a function over a feasible set, at the optimal solution,
 the direction in which the function increases the fastest increase must
 be parallel to the normal of the feasible set.
 The level sets of our objective function are hyperplanes, and our feasible
 set is a polytope.
 Clearly, maxima will occur at a vertex of the polytope.
 At this point, we will be able to combine the normals (which define the
 facets adjacent to the vertex) linearly, with non-negative coefficients
 to arrive at the normal of the objective function.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L\left(x,\lambda\right)=c^{T}x-\sum_{i}\lambda_{i}a_{i}^{T}x
\]

\end_inset


\end_layout

\begin_layout Standard
In the dual formulation of 
\begin_inset Formula $\alpha^{\star}$
\end_inset

, we want to minimize the sum of cliques, such that the sum of cliques over
 any single vertex is at least one.
 Let 
\begin_inset Formula $\left\{ y_{i}\in\left[0,1\right]\right\} _{N}$
\end_inset

, where 
\begin_inset Formula $N$
\end_inset

 is the total number of cliques in the graph, and each 
\begin_inset Formula $y_{i}$
\end_inset

 corresponds to a clique.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha^{\star}=q^{\star}=\min_{y}\left(\sum_{i=1}^{N}y_{i}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
under the constraints 
\begin_inset Formula $y_{i}\in\left[0,1\right]$
\end_inset

.
 For every vertex 
\begin_inset Formula $v$
\end_inset

, 
\begin_inset Formula 
\[
\sum_{y_{i}\ni v}y_{i}\geq1
\]

\end_inset


\end_layout

\begin_layout Standard
Observe that if we form the associated integer linear program by restricting
 
\begin_inset Formula $y_{i}\in\left\{ 0,1\right\} $
\end_inset

, then we will have the clique covering problem, so 
\begin_inset Formula $\alpha^{\star}$
\end_inset

 may also be also called 
\begin_inset Formula $q^{\star}$
\end_inset

.
 Thus, 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 are an example of dual integer linear programs which do not agree.
\end_layout

\begin_layout Section*
\begin_inset Formula $\vartheta$
\end_inset

 as a Semidefinite Program
\end_layout

\begin_layout Standard
Semidefinite programs are 
\begin_inset Quotes eld
\end_inset

vector relaxations.
\begin_inset Quotes erd
\end_inset

 Instead of integers or real numbers, our variables range over vectors.
 In the case of 
\begin_inset Formula $\vartheta$
\end_inset

, these are our Orthonormal Representations.
 These collections of vectors are in perfect correspondence with positive
 semidefinite matricies.
 A symmetric matrix 
\begin_inset Formula $m\in\mathbb{R}^{n}\times\mathbb{R}^{n}$
\end_inset

 is called positive semidefinite if 
\begin_inset Formula $\left\langle x,mx\right\rangle \geq0$
\end_inset

 for all 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

.
 If this is the case, then we can define a new inner product as 
\begin_inset Formula $\left(x,y\right)=\left\langle x,my\right\rangle $
\end_inset

, sans positive-definiteness.
 To look at the entries of 
\begin_inset Formula $m$
\end_inset

, we can set 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 to be members of the basis.
 The 
\begin_inset Formula $ij^{th}$
\end_inset

 element of 
\begin_inset Formula $m$
\end_inset

 will be 
\begin_inset Formula $m_{ij}=\left\langle e_{i},me_{j}\right\rangle =\left(e_{i},e_{j}\right)$
\end_inset

.
 In other words, the matrix 
\begin_inset Formula $m$
\end_inset

 tells us how our new inner product behaves on our basis elements.
 The argument also works conversely, so positive semidefinite matricies
 are in 
\begin_inset Formula $1-1$
\end_inset

 correspondence with gram matricies.
\end_layout

\begin_layout Standard
A semidefinite program is of the form
\begin_inset Formula 
\[
Min{}_{X}\left(C\cdot X\right)=Min_{X}\left(TR\left(CX\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
such that 
\begin_inset Formula $X$
\end_inset

 satisfies
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X\succeq0,\left\{ D_{i}\cdot X=p_{i}\right\} _{i=1}^{k}
\]

\end_inset


\end_layout

\begin_layout Standard
In such a problem, we will be supplied with the 
\begin_inset Formula $k$
\end_inset

 
\begin_inset Formula $n\times n$
\end_inset

 symmetric matricies 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\left\{ D_{i}\right\} $
\end_inset

, along with real numbers 
\begin_inset Formula $\left\{ p_{i}\right\} _{i=1}^{k}$
\end_inset

.
\end_layout

\begin_layout Standard
The notation 
\begin_inset Formula $X\succeq0$
\end_inset

 states that 
\begin_inset Formula $X$
\end_inset

 be positive semidefinite.
 As in the linear programming case, our objective function is some linear
 function of the variables.
 We are allowed linear equality constraints on the entries of 
\begin_inset Formula $X$
\end_inset

, which are in effect constraints of linear combinations of dot products
 of our vector assignments.
\end_layout

\begin_layout Standard
Rather than supplying the linear equality constraints 
\begin_inset Formula $\left\{ D_{i}\cdot X=p_{i}\right\} _{i=1}^{k}$
\end_inset

, we could express the feasible set directly, in the form 
\begin_inset Formula $X\left(y\right)=G_{0}+\sum_{i=1}^{m}\left(y_{i}G_{i}\right)$
\end_inset

 ard requiring that 
\begin_inset Formula $X\left(y\right)\succeq0$
\end_inset

.
 Set 
\begin_inset Formula $d_{i}=C\cdot G_{i}$
\end_inset

.
 We obtain one equivalent formulation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Min_{y}d^{T}y
\]

\end_inset


\end_layout

\begin_layout Standard
such that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X\left(y\right)\succeq0
\]

\end_inset


\end_layout

\begin_layout Theorem
We can express 
\begin_inset Formula $\vartheta$
\end_inset

 as a semidefinite program.
\end_layout

\begin_layout Proof
Consider the following program, which computes the vector chromatic number:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\frac{-1}{\vartheta-1}=\mbox{min}_{m}\left(\alpha\right)
\]

\end_inset


\end_layout

\begin_layout Proof
such that 
\begin_inset Formula $m\succeq0$
\end_inset

 and
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
m_{ij}=\alpha\mbox{ if }v_{i}\sim v_{j}
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
m_{ii}=1
\]

\end_inset


\end_layout

\begin_layout Proof
It's clear that the program computes the vector chromatic number, but we
 have to justify that it is a semidefinite optimization problem, as defined
 above.
 In particular, the constraint 
\begin_inset Formula $m_{ij}=\alpha$
\end_inset

 is problematic, since 
\begin_inset Formula $\alpha$
\end_inset

 is our objective function.
 We can replace these constraints with a set of constraints which requires
 that all 
\begin_inset Formula $m_{ij}$
\end_inset

 where 
\begin_inset Formula $v_{i}\sim v_{j}$
\end_inset

 be equal.
 To do this, order the edges 
\begin_inset Formula $e_{1},e_{2},\dots,e_{E\left(G\right)}$
\end_inset

 arbitrarily, and require that 
\begin_inset Formula $m_{e_{i}}=m_{e_{i+1}}$
\end_inset

.
 Rather than minimizing 
\begin_inset Formula $\alpha$
\end_inset

, we can minimize 
\begin_inset Formula $m_{e_{1}}$
\end_inset

.
 Then we arrive at an equivalent problem, which is clearly semidefinite.
 Let 
\begin_inset Formula $y\in\mathbb{R}^{k+1}$
\end_inset

, and 
\begin_inset Formula $d=\left(1,0,0,\dots\right)$
\end_inset

 Then
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\frac{-1}{\vartheta-1}=\min\left(d^{T}y\right)
\]

\end_inset


\end_layout

\begin_layout Proof
subject to
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
I+y_{1}A+\sum_{i=2}^{k}y_{i}D_{i}=M\left(y\right)\succeq0
\]

\end_inset


\end_layout

\begin_layout Proof
Here, 
\begin_inset Formula $J$
\end_inset

 is the matrix of all 
\begin_inset Formula $1$
\end_inset

's, 
\begin_inset Formula $A\in\mathbb{R}^{n\times n}$
\end_inset

 is the adjacency matrix of the graph, which which has the 
\begin_inset Formula $i,j^{th}$
\end_inset

 entry 
\begin_inset Formula $1$
\end_inset

 when 
\begin_inset Formula $v_{i}\sim v_{j}$
\end_inset

, and 
\begin_inset Formula $0$
\end_inset

 otherwise.
 
\begin_inset Formula $D_{i}$
\end_inset

's are the collection of matricies with 
\begin_inset Formula $1$
\end_inset

 entry of 
\begin_inset Formula $1$
\end_inset

 in the 
\begin_inset Formula $i,j^{th}$
\end_inset

 entry where 
\begin_inset Formula $v_{i}\not\sim v_{j}$
\end_inset

.
\end_layout

\begin_layout Proof
Observe that if we scale the solution to the vector coloring problem so
 that each vector has length 
\begin_inset Formula $\sqrt{\vartheta-1}$
\end_inset

, then this will imply that the dot products of adjacent vectors are 
\begin_inset Formula $-1$
\end_inset

.
 And conversely, if we have a solution to the scaled problem we can recover
 a solution to the original problem.
 Hence,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\vartheta-1=\min\left(\frac{1}{n}I\cdot Z\right)
\]

\end_inset


\end_layout

\begin_layout Proof
subject to the constraints that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
Z_{11}=Z_{22}=\dots=Z_{n,n};\mbox{ }Z_{ij}=-1\mbox{ }\mbox{if }v_{i}\sim v_{j};Z\succeq0
\]

\end_inset


\end_layout

\begin_layout Definition
Suppose we have semidefinite program in the second formulation.
 Its dual is defined
\begin_inset CommandInset citation
LatexCommand cite
key "vand96"

\end_inset

 as
\end_layout

\begin_layout Definition
\begin_inset Formula 
\[
Max_{Z}\left(-G_{0}\cdot Z\right)
\]

\end_inset


\end_layout

\begin_layout Definition
such that
\end_layout

\begin_layout Definition
\begin_inset Formula 
\[
Z\succeq0,\left\{ G_{i}\cdot Z=d_{i}\right\} _{i=1}^{m}
\]

\end_inset


\end_layout

\begin_layout Standard
Just as the linear programming duality arises from finding bounds on the
 optimal values for the primal program, the duality in semidefinite programming
 also arises in this way, since
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
d^{T}y+Z\cdot G_{0}=\sum_{i=1}^{m}\left(Z\cdot G_{i}\right)y_{i}+Z\cdot G_{0}=Z\cdot G\left(y\right)\geq0
\]

\end_inset


\end_layout

\begin_layout Standard
so
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
d^{T}y\geq-G_{0}\cdot Z
\]

\end_inset


\end_layout

\begin_layout Standard
In fact, we have equality, assuming that there is a positive definite feasible
 matrix in either the primal or dual problem.
 We can see that this occurs exactly when 
\begin_inset Formula $Z\cdot G\left(y\right)=0$
\end_inset

.
 (!) This is the antiblocker theorem.
\end_layout

\begin_layout Theorem
We can express 
\begin_inset Formula $\vartheta$
\end_inset

 as a semidefinite program
\begin_inset CommandInset citation
LatexCommand cite
key "lov78"

\end_inset

:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\[
\vartheta=\max\left(J\cdot B\right)
\]

\end_inset


\end_layout

\begin_layout Theorem
subject to 
\begin_inset Formula 
\[
b_{ij}=0\mbox{ if }v_{i}\sim v_{j};Tr\left(B\right)=1
\]

\end_inset


\end_layout

\begin_layout Theorem
Where 
\begin_inset Formula $J$
\end_inset

 is the 
\begin_inset Formula $1$
\end_inset

's matrix (so 
\begin_inset Formula $J\cdot B$
\end_inset

) is the sum of entries in 
\begin_inset Formula $B$
\end_inset

.
\end_layout

\begin_layout Proof
We will take the dual of the semidefinite program is Theorem 
\begin_inset Formula $27$
\end_inset

.
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\frac{-1}{\vartheta-1}=\max_{Z}\left(-I\cdot Z\right)=\max_{Z}\left(-Tr\left(Z\right)\right)
\]

\end_inset


\end_layout

\begin_layout Proof
subject to the constraints
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
A\cdot Z=1,\left\{ D_{i}Z=0\right\} _{i=1}^{k}
\]

\end_inset


\end_layout

\begin_layout Proof
The second set of constraints implies that 
\begin_inset Formula $Z_{ij}$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 whenever 
\begin_inset Formula $v_{i}\sim v_{j}$
\end_inset

.
 It follows that 
\begin_inset Formula $J\cdot Z=\left(I+A\right)\cdot Z=Tr\left(Z\right)+A\cdot Z$
\end_inset

.
 Thus, for the optimal 
\begin_inset Formula $Z^{\star}$
\end_inset

, 
\begin_inset Formula $J\cdot Z=1+\frac{1}{\vartheta-1}=\frac{\vartheta}{\vartheta-1}$
\end_inset

.
 Thus, if we scale up all the entries in 
\begin_inset Formula $Z$
\end_inset

 by 
\begin_inset Formula $\vartheta-1$
\end_inset

, we arrive at the desired semidefinite program.
\end_layout

\begin_layout Remark
The feasible set 
\begin_inset Formula $B$
\end_inset

 is related to the set of Orthonormal Representations in the 
\begin_inset Formula $max$
\end_inset

 definition of 
\begin_inset Formula $\vartheta$
\end_inset

.
 Given an O.R., one can compute its gram matrix and obtain a PSD matrix 
\begin_inset Formula $C$
\end_inset

 such that 
\begin_inset Formula $Tr\left(C\right)=n$
\end_inset

.
 It is not obvious how to interpret the objective function, 
\begin_inset Formula $J\cdot B$
\end_inset

 as relating to 
\begin_inset Formula $\sum_{i\in V\left(G\right)}\left(\psi^{T}v_{i}\right)^{2}$
\end_inset

.
 Lovasz proves
\begin_inset CommandInset citation
LatexCommand cite
key "lov78"

\end_inset

 our max definition from the SDP.
 In the course of his proof, he appears(!) to show that the optimal handle
 is always 
\begin_inset Formula $\psi^{\star}=\frac{\sum v_{i}}{\left|\sum v_{i}\right|}$
\end_inset

.
 
\end_layout

\begin_layout Section*
A consequence of the 'max' semidefinite program (!)
\end_layout

\begin_layout Standard
Our first, 'min' semidefinite program resulted from our ability to phrase
 the min definition of 
\begin_inset Formula $\vartheta$
\end_inset

 in a way that got rid of 
\begin_inset Formula $\psi$
\end_inset

 and resulted in a linear objective function.
 This was the vector coloring problem.
 It would be nice to be able to remove 
\begin_inset Formula $\psi$
\end_inset

 from the max definition and arrive at the max semidefinite program through
 an arrangement of vectors, but there are difficulties.
 The following inequality comes from 
\begin_inset CommandInset citation
LatexCommand cite
key "lov78"

\end_inset

, and is a nice application of Cauchy Schwarz.
 Let 
\begin_inset Formula $w_{i}$
\end_inset

 be some rescaling of 
\begin_inset Formula $v_{i}$
\end_inset

 such that 
\begin_inset Formula $\sum_{i}w_{i}^{2}=1$
\end_inset

.
 
\begin_inset Formula 
\[
\left(\sum_{i=1}^{n}\left(\psi^{T}v_{i}\right)^{2}\right)=\left(\sum_{i=1}^{n}w_{i}^{2}\right)\left(\sum_{i=1}^{n}\left(\psi^{T}v_{i}\right)^{2}\right)\geq\left(\sum_{i=1}^{n}\left|w_{i}\right|\psi^{T}v_{i}\right)^{2}=\left(\sum_{i=1}^{n}\psi^{T}w_{i}\right)^{2}\leq\left(\sum_{i=1}^{n}w_{i}\right)^{2}=\sum_{i,j}w_{i}^{T}w_{j}
\]

\end_inset


\end_layout

\begin_layout Standard
The first inequality can be made to hold if we select our scaling such that
 
\begin_inset Formula $w_{i}^{2}\propto\left(\psi^{T}v_{i}\right)^{2}$
\end_inset

.
 The second inequality can be made into equality if we select 
\begin_inset Formula $\psi$
\end_inset

 to be a unit vector in the direction of 
\begin_inset Formula $\sum_{i=1}^{n}w_{i}$
\end_inset

.
 Mysteriously, we can make both of these true at the same time.
\end_layout

\begin_layout Section*
The Ellipsoid Method: Convex Optimization is Polynomial Time
\end_layout

\begin_layout Standard
There is a general technique to optimize any linear objective function over
 any convex feasible set.
 This technique is slow in practice, but runs in polynomial time.
 In particular, suggests that semidefinite optimization problems may be
 solved efficiently.
 A correct analysis of the technique is complicated because we need to take
 rounding errors into account.
 Since the optimal may not have a terminating decimal representation, we
 need to allow solutions which only approximate that optimal value up to
 
\begin_inset Formula $\epsilon$
\end_inset

, and we need to show that this approximation is polynomial in 
\begin_inset Formula $\log\left(\frac{1}{\epsilon}\right)$
\end_inset

.
 Furthermore, we need to take into account rounding errors in the representation
 convex set itself.
 The following description comes from 
\begin_inset CommandInset citation
LatexCommand cite
key "reed01"

\end_inset


\end_layout

\begin_layout Standard
In the following, a convex body will be modeled as an oracle which, given
 a point, will either assert that the point is in the convex body, or else
 it will return a separating hyperplane between the point and the body.
\end_layout

\begin_layout Standard
First, suppose that we have some convex body 
\begin_inset Formula $C$
\end_inset

 and we wish to optimize 
\begin_inset Formula $\max_{x\in C}\left(a^{T}x\right)$
\end_inset

 with 
\begin_inset Formula $\left\Vert a\right\Vert =1$
\end_inset

 Suppose we have a function 
\begin_inset Formula $sample\left(C\right)$
\end_inset

 which takes a convex body and returns a point in that set or correctly
 asserts that 
\begin_inset Formula $C$
\end_inset

 is empty.
 We must also assume that 
\begin_inset Formula $C$
\end_inset

 contains a ball 
\begin_inset Formula $b_{0,r}$
\end_inset

 and is contained in a ball 
\begin_inset Formula $B_{0,R}$
\end_inset

.
 Then we can solve the optimization problem by means of a binary search.
 We know that 
\begin_inset Formula $0\leq\max_{x}\left\{ a^{T}x\mid x\in C\right\} \leq R$
\end_inset

.
 We can sample 
\begin_inset Formula $x_{0}$
\end_inset

 and calculate 
\begin_inset Formula $a^{T}x_{0}$
\end_inset

.
 Then, we consider 
\begin_inset Formula $C_{1}=C\cap\left\{ x\mid a^{T}x\geq\frac{a^{T}x_{0}+R}{2}\right\} $
\end_inset

.
 We can sample again, and if we find a point, 
\begin_inset Formula $x_{1}$
\end_inset

, we known that 
\begin_inset Formula $\max_{x\in C}\left(a^{T}x\right)\geq a^{T}x_{1}$
\end_inset

.
 Otherwise, we know 
\begin_inset Formula $\max_{x\in C}\left(a^{T}x\right)\leq a^{T}x_{1}$
\end_inset

.
 In either case, we have halved the range of possible values for 
\begin_inset Formula $\max_{x\in C}\left(a^{T}x\right)$
\end_inset

, so it only takes one more iteration of the algorithm to calculate one
 more bit of the output.
 In other words, it is polynomial-time.
\end_layout

\begin_layout Standard
Thus, if we can find points inside a convex body, 
\begin_inset Formula $C$
\end_inset

, we can optimize over 
\begin_inset Formula $C$
\end_inset

 in polynomial time.
 The ellipsoid method is a technique to find a point inside a convex body.
 If we know nothing about 
\begin_inset Formula $C$
\end_inset

, we are left guessing at points and the problem is hopeless.
 We must assume we are told our convex body is contained in a ball 
\begin_inset Formula $S\left(0,B\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The ellipsoid method works via ellipsoids whose volumes shrink by constant
 factors.
 The first ellipsoid is the ball 
\begin_inset Formula $S\left(0,B\right).$
\end_inset

 Then, we guess a point at the center of the ellipsoid, 
\begin_inset Formula $0$
\end_inset

.
 If this fails, the oracle will return a separating hyperplane to us.
 Call the halfspace containing 
\begin_inset Formula $C$
\end_inset

, 
\begin_inset Formula $H$
\end_inset

.
 We construct a new ellipsoid, which is the smallest ellipsoid containing
 
\begin_inset Formula $S\left(0,B\right)\cap H$
\end_inset

.
 Generally, we guess at the center of the current ellipsoid, and if we're
 wrong, use the resulting hyperplane to construct an new ellipsoid with
 less volume by a constant (depending on the dimension) factor.
 These ellipsoids do not work in practice, but a simple (if you ignore precision
) way to argue that optimization over a convex body can be achieved in polynomia
l time.
\end_layout

\begin_layout Standard
The ellipsoid method requires a separation oracle.
 For a semidefinite programming problem, we must be able to decide whether
 or not a point is in the feasible set, and if not, then we should be able
 to compute a separating hyperplane.
 Specifically, we need to be able to determine whether a symmetric matrix
 is positive semidefinite or not.
 First, we diagonalize the matrix is 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

 time, thereby writing 
\begin_inset Formula $M=UDU^{T}$
\end_inset

 where 
\begin_inset Formula $U$
\end_inset

 is orthonormal and 
\begin_inset Formula $D$
\end_inset

 is diagonal.
 
\begin_inset Formula $M$
\end_inset

 is positive semidefinite exactly when 
\begin_inset Formula $D$
\end_inset

 has non-negative entries.
 If 
\begin_inset Formula $D$
\end_inset

 has a negative entry, then we can find a vector 
\begin_inset Formula $c$
\end_inset

 such that 
\begin_inset Formula $U^{T}c$
\end_inset

 is the basis vector associated with that entry, and 
\begin_inset Formula $c^{T}UDU^{T}c=c^{T}Mc<0$
\end_inset

.
 Since 
\begin_inset Formula $c^{T}Mc=\sum c_{i}c_{j}m_{ij}$
\end_inset

, this is a linear functional on 
\begin_inset Formula $M$
\end_inset

 which separates 
\begin_inset Formula $M$
\end_inset

 from the positive cone.
\end_layout

\begin_layout Section*
Interior Point Methods
\end_layout

\begin_layout Standard
Linear programming problems are usually solved by the simplex method, where
 the vertices of the feasible polytope are examined systematically.
 Our semidefinite programs do not have polytopes as feasible sets, so there
 would be infinitely many vertices to check.
 Instead, we use interior point methods, where we search the interior of
 the feasible set.
 But first we rephrase the problem to contain both the primal and dual problems.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min\left(\left(c^{T}y\right)+TR\left(F_{0}Z\right)\right)=F\left(y\right)\cdot Z
\]

\end_inset


\end_layout

\begin_layout Standard
subject to the fact that 
\begin_inset Formula $F\left(y\right)$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 are primal and dual feasible respectively.
 
\end_layout

\begin_layout Standard
We know by the strong duality theorem for linear programming that this minimum
 is actually 
\begin_inset Formula $0$
\end_inset

.
 By finding the location where this occurs, we can find the optimal values
 
\begin_inset Formula $F\left(y\right)$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Standard
To do this, we assign a barrier function which is infinite at the boundary
 of both feasible sets.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi\left(Z\right)=\begin{cases}
\log\left(\det Z^{-1}\right) & \mbox{ if }Z>0\\
\infty & else
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
This function is convex (why?), so it has a unique minimum, known as the
 analytic center.
 We can find this analytic center using Newton's Method
\begin_inset CommandInset citation
LatexCommand cite
key "boy04"

\end_inset

: 
\end_layout

\begin_layout Enumerate
Start with an arbitrary point, 
\begin_inset Formula $x$
\end_inset

.
 (in our case, this is some feasible positive definite matrix 
\begin_inset Formula $Z$
\end_inset

.)
\end_layout

\begin_layout Enumerate
Calculate the tangent line for which the second order approximation for
 
\begin_inset Formula $f$
\end_inset

 is minimized (it happens to be
\begin_inset Formula $-\nabla^{2}f\left(x\right)^{-1}\nabla f\left(x\right)$
\end_inset

)
\end_layout

\begin_layout Enumerate
Find the point 
\begin_inset Formula $x^{\prime}$
\end_inset

 along this tangent line for which 
\begin_inset Formula $f\left(x^{\prime}\right)$
\end_inset

.
 This is just a 
\begin_inset Formula $1$
\end_inset

-dimensional search and can be performed efficiently.
\end_layout

\begin_layout Standard
Newton's Method has good convergence results when 
\begin_inset Formula $f$
\end_inset

 is self-concordant: 
\begin_inset Formula $\left|f^{\prime\prime\prime}\left(x\right)\right|\leq2f^{\prime\prime}\left(x\right)^{\frac{3}{2}}$
\end_inset

, and our barrier function is such a function.
\end_layout

\begin_layout Standard
The analytic center of the primal and dual feasible sets can be extended
 to a 
\begin_inset Quotes eld
\end_inset

central path
\begin_inset Quotes erd
\end_inset

 to the optimal points
\begin_inset CommandInset citation
LatexCommand cite
key "vand96"

\end_inset

.
 Specifically, for each 
\begin_inset Formula $\gamma>0$
\end_inset

, we consider the set 
\begin_inset Formula $F\left(y\right)>0,$
\end_inset

 and 
\begin_inset Formula $d^{T}y=\gamma$
\end_inset

.
 This is a subset of the feasible set, and for 
\begin_inset Formula $\gamma$
\end_inset

 near the optimum, it is nonempty.
 Therefore, we can calculate its analytic center.
 Doing so, we obtain a curve parameterized by 
\begin_inset Formula $\gamma$
\end_inset

 of feasible matrices 
\begin_inset Formula $F\left(y\right)$
\end_inset

 which leads to the optimal feasible matrix.
\end_layout

\begin_layout Proposition
If 
\begin_inset Formula $F\left(y\right)$
\end_inset

 is on the central path, there will be a corresponding dual feasible matrix
 
\begin_inset Formula $Z$
\end_inset

 on the dual central path.
\end_layout

\begin_layout Proof
\begin_inset CommandInset citation
LatexCommand cite
key "vand96"

\end_inset

 If 
\begin_inset Formula $F\left(y\right)$
\end_inset

 is on the central path, with parameter 
\begin_inset Formula $\gamma$
\end_inset

, then 
\begin_inset Formula $F\left(y\right)$
\end_inset

 is the solution to the optimization
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
y^{\star}=argmin\left(\log\left(\det F\left(y\right)\right)^{-1}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
such that 
\begin_inset Formula $F\left(y\right)\succeq0$
\end_inset

 and 
\begin_inset Formula $c^{T}y=\gamma$
\end_inset

.
 We can ignore the first condition, because our objective function is 
\begin_inset Formula $\infty$
\end_inset

 when it is violated.
 The method of Lagrange Multipliers says that a convex function can only
 be minimized when the gradient of the objective function is parallel to
 the constraint.
 To perform partial derivative 
\begin_inset Formula $\frac{\partial}{\partial y_{y}}$
\end_inset

 on the objective function, we need to use Jacobi's Formula.
 (! wikipedia) In particular, for invertible 
\begin_inset Formula $A$
\end_inset

 we have 
\begin_inset Formula $\frac{d}{dt}\det A\left(t\right)=Tr\left(\det\left(A\right)A^{-1}\frac{dA\left(t\right)}{dt}\right)$
\end_inset

.
 In our case, this yeilds
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\frac{1}{\det F\left(y^{\star}\right)^{-1}}Tr\left(\det F\left(y^{\star}\right)^{-1}F\left(y^{\star}\right)\frac{dF\left(y\right)^{-1}}{dy}\right)=Tr\left(F\left(y^{\star}\right)\frac{dF\left(y\right)^{-1}}{dy_{i}}\right)=Tr\left(F\left(y^{\star}\right)^{-1}F_{i}\right)=\lambda c_{i}
\]

\end_inset


\end_layout

\begin_layout Proof
Or, in other words, 
\begin_inset Formula $\frac{F\left(y\right)^{-1}}{\lambda}$
\end_inset

 is dual-feasible.
 It's clear that the duality gap between 
\begin_inset Formula $Tr\left(\frac{F\left(y^{\star}\right)^{-1}}{\lambda}\cdot F\left(y^{\star}\right)\right)=\frac{n}{\lambda}$
\end_inset

.
 Also, its claimed (! the proof appears to be missing from 
\begin_inset CommandInset citation
LatexCommand cite
key "vand96"

\end_inset

) that 
\begin_inset Formula $\frac{F\left(y^{\star}\right)^{-1}}{\lambda}$
\end_inset

 is on the central path.
 Thus, the two central paths can be paremerized by the dual gap, and the
 corresponding points are near-inverses.
\end_layout

\begin_layout Standard
Next, we introduce a distance function to serve as a penalty for deviating
 from the central path.
\begin_inset Formula 
\[
\psi\left(y,Z\right)\coloneqq-\log\det\left(F\left(y\right)Z\right)+\log\det F\left(y^{\star}\right)Z^{\star}=-\log\det F\left(y\right)Z+n\log Tr\left(F\left(y\right)Z\right)-n\log n
\]

\end_inset


\end_layout

\begin_layout Standard
This can actually be rewritten independently of 
\begin_inset Formula $y^{\star}$
\end_inset

 by using the fact that 
\begin_inset Formula $F\left(y^{\star}\right)Z^{\star}=\frac{\eta}{n}I$
\end_inset

.
\end_layout

\begin_layout Standard
Finally, we introduce a potential which takes into account 
\begin_inset Formula $\psi$
\end_inset

, the deviation from the central path and 
\begin_inset Formula $\eta$
\end_inset

, the duality gap.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi\coloneqq\nu\sqrt{n}\log\left(F\left(y\right)\cdot Z\right)+\psi\left(y,Z\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We can apply a modified Newton's Method to minimize 
\begin_inset Formula $\phi$
\end_inset

.
 First, we need a direction to search.
 This is the time-limiting step.
\end_layout

\begin_layout Section*
Kochen-Specker Sets
\end_layout

\begin_layout Standard
It is possible to establish contextuality without probabilities, and in
 a way that is independent of the quantum state.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename merminSquare.eps
	scale 50

\end_inset


\end_layout

\begin_layout Standard
Mermin's square is a special arrangement of observables on pairs of qubits
 which provides an example of a state-independent proof of contextuality.
 In fact, there are no satisfying assignments to the observables at all.
 In the diagram, the pauli operators 
\begin_inset Formula $X,Y,Z$
\end_inset

 satisfy 
\begin_inset Formula $X^{2}=Y^{2}=Z^{2}=I$
\end_inset

 and 
\begin_inset Formula $XY=iZ$
\end_inset

.
 Each row and column consists of 
\begin_inset Formula $3$
\end_inset

 commuting observables, and the products of each row and column are 
\begin_inset Formula $I$
\end_inset

, except for the last column, where the product is 
\begin_inset Formula $-I$
\end_inset

.
 This means that any row and any column can be measured and that the product
 of the outcomes is 
\begin_inset Formula $1$
\end_inset

 for each row/column, except 
\begin_inset Formula $-1$
\end_inset

 for the last column.
\end_layout

\begin_layout Standard
We should draw the exclusivity graph for this well-studied example.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename critical_graphs/merminSquareGraph
	scale 40

\end_inset


\end_layout

\begin_layout Standard
Here, we have 
\begin_inset Formula $6$
\end_inset

 contexts, 
\begin_inset Formula $R1,R2,R3,C1,C2,C3$
\end_inset

 and each context has 
\begin_inset Formula $4$
\end_inset

 possible outcomes.
 For example, for the node 
\begin_inset Formula $R1-001$
\end_inset

, the '
\begin_inset Formula $R1$
\end_inset

' indicates that we chose to measure the first row, 
\begin_inset Formula $\left\{ IZ,ZI,ZZ\right\} $
\end_inset

, simultaneously.
 The '
\begin_inset Formula $001$
\end_inset

' means that 
\begin_inset Formula $IZ$
\end_inset

 measured 
\begin_inset Formula $-1$
\end_inset

, 
\begin_inset Formula $ZI$
\end_inset

 measured 
\begin_inset Formula $-1$
\end_inset

 and 
\begin_inset Formula $ZZ$
\end_inset

 measured 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
This graph was created by adding edges where experiments disagree.
 For example, there is an edge between 
\begin_inset Formula $R3-010$
\end_inset

 and 
\begin_inset Formula $C2-100$
\end_inset

 because the measurement 
\begin_inset Formula $ZX$
\end_inset

 is 
\begin_inset Formula $+1$
\end_inset

 in the first node and 
\begin_inset Formula $-1$
\end_inset

 in the second.
 After a measurement, the state vectors will be orthogonal to one another,
 because the first will be in the positve eigenspace of 
\begin_inset Formula $ZX$
\end_inset

 and the second will be in the negative.
 Hence, two 'incompatible' vectors will be orthogonal.
 The graph should really be drawn according to orthogonalities, so we need
 to be careful and consider the reverse: Do all orthogonalities come from
 observables? Generally, it's true that there is some observable to distinguish
 between any two orthogonal states: We can define an observable by its positive
 and negative eigenvectors- which are the ones we want to distinguish.
 In the Mermin Square example, it happens that there are no more orthogonality
 relations than the ones found by disagreements in measurement, but this
 is not necessarily the case.
 If we dropped 
\begin_inset Formula $YY$
\end_inset

 from the measurements, so that 
\begin_inset Formula $C3=\left\{ ZZ,XX\right\} $
\end_inset

 and 
\begin_inset Formula $R3=\left\{ XZ,ZX\right\} $
\end_inset

, then the outcome 
\begin_inset Formula $C3-11$
\end_inset

 would be orthogonal to 
\begin_inset Formula $R3-11$
\end_inset

, we would have no measurement in our list on which these two disagree.
 Therefore, I stress that one must actually compute eigenvectors and their
 orthogonalities to draw these graphs properly.
 The script in grenerateGraphFromObservables.py does this.
\end_layout

\begin_layout Standard
The computer (see the file MerminSquareGraph.py) indicates that 
\begin_inset Formula 
\[
\vartheta\left(G\right)=6.0,\alpha\left(G\right)=5
\]

\end_inset


\end_layout

\begin_layout Standard
The interpretation of this is slightly different than in the CHSH example.
 In that example, we did not have a concrete set of observables, and were
 instead trying to find a bound over a class of them which satisfied certain
 relations.
 Here, the Mermin square defines a specific set of observables.
 In the CHSH example, we had to take a subset which corresponds to a weighting
 of the nodes.
 Here, our graph is the vectors of the experiment themselves.
\end_layout

\begin_layout Standard
The fact that 
\begin_inset Formula $\alpha\left(G\right)<q\left(G\right)=6$
\end_inset

, indicates that there is no independent set which can meet every clique.
 We've said that classical states (global assignments of outcomes) correspond
 to independent sets.
 Each context corresponds to a clique.
 Thus, there is no assignment to the observables which fits into every context.
 In other words, we have strong contextuality.
\end_layout

\begin_layout Standard
In fact, it appears that you can always detect (strong) contextuality in
 this way: A section corresponds to a vertex.
 A global section corresponds to an independent set that meets each clique.
 If the vertex of a section section can be extended to an independent set
 which meets ever clique, that section can be extended to a global section,
 and conversely as well.
\end_layout

\begin_layout Standard
Given a measurement scenario, our graphs are precisely the complement of
 the graph whose vertices are the sections over each context, and whose
 edges are compatibility relations.
 If we draw these graphs, and calculate independence numbers, it seems that
 we can completely solve the problem of detecting contextuality.
 This is implemented in generateGraphFromObservables.py
\end_layout

\begin_layout Standard
Since we no longer have an optimization problem, 
\begin_inset Formula $\vartheta$
\end_inset

 is less useful here.
 The value 
\begin_inset Formula $\vartheta\left(G\right)=6$
\end_inset

 indicates this configuration of vectors in 
\begin_inset Formula $4$
\end_inset

 dimensions is optimal.
 Recall that 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 can be regarded as the maximum sum of probabilities at each vertex over
 all possible orthonormal representations.
 Our observables' eigenvectors form an orthonormal representation whose
 sum of probabilities is 
\begin_inset Formula $6$
\end_inset

, since we are guaranteed a successful measurement in each of the 
\begin_inset Formula $6$
\end_inset

 contexts.
 
\end_layout

\begin_layout Standard
It would be interesting to see what solution the semidefinite program returns.
 This program should return the positive semidefinite matrix 
\begin_inset Formula $B$
\end_inset

 whose normalized Cholesky decomposition gives the orthonormal representation,
 and the handle is the weighted sum of cholesky vectors.
 The maximum of a linear functional over a convex set occurs at the boundary.
 Thus, I would expect that 
\begin_inset Formula $det\left(B\right)=0$
\end_inset

, which implies that the cholesky vectors should never be full dimension.
\end_layout

\begin_layout Section*
The Relationship Between Orthogonality Graphs and Possibilistic Empirical
 Models
\end_layout

\begin_layout Standard
Contextuality has also been studied via cohomology
\begin_inset CommandInset citation
LatexCommand cite
key "abr98"

\end_inset

.
 In this setup, we begin with a measurement scenario: 
\begin_inset Formula $\left\langle X,M,O\right\rangle $
\end_inset

, where 
\begin_inset Formula $X$
\end_inset

 is a collection of abstract measurements (not necessarily maximal), 
\begin_inset Formula $M\subset2^{X}$
\end_inset

, a collection of contexts, which define those measurements which are assumed
 to be co-measureable, and 
\begin_inset Formula $O$
\end_inset

, a collection of outcomes.
 We assume that 
\begin_inset Formula $M$
\end_inset

 has a simplicial structure (a sub-collection of co-measureable measurements
 must again be co-measureable).
 This setup is quite general and does not assume that the measurements 
\begin_inset Formula $X$
\end_inset

 have any particular structure, such as that they are quantum measurements
 defined by hermitian matricies.
\end_layout

\begin_layout Standard
We then define a sheaf of events: 
\begin_inset Formula $\mathscr{E}:\left(2^{X}\right)^{op}\to SET$
\end_inset

, where 
\begin_inset Formula $\mathscr{E}\left(U\right)\coloneqq O^{U}$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "car17"

\end_inset

.
 For every collection of measurements, the sheaf of events describes the
 possible outcomes for these measurements.
 There is no restriction to these outcomes, and we only define the presheaf
 of events so that we can define the possibilistic empirical models.
 These define the possible outcomes for our model in the same way that the
 orthogonality graph describes which outcomes may occur.
\end_layout

\begin_layout Standard
A possibilistic empirical model is a sub presheaf of the sheaf of events
 satisfying
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathscr{S}\left(C\right)\neq\emptyset\forall C\in M$
\end_inset


\end_layout

\begin_layout Enumerate
The restriction map 
\begin_inset Formula $\rho_{U}^{U^{\prime}}\coloneqq\mathscr{S}\left(U^{\prime}\right)\to\mathscr{S}\left(U^{\prime}\right)::s\mapsto s\mid_{U}$
\end_inset

 is surjective when 
\begin_inset Formula $U\subset U^{\prime}\subset C$
\end_inset

 for some 
\begin_inset Formula $C\in M$
\end_inset

.
\end_layout

\begin_layout Enumerate
Every family 
\begin_inset Formula $\left\{ s_{C}\in\mathscr{S}\left(C\right)\right\} _{C\in M}$
\end_inset

 such that 
\begin_inset Formula $s_{C}\mid_{C\cap C^{\prime}}=s_{C^{\prime}}\mid_{C\cap C^{\prime}}$
\end_inset

 for all 
\begin_inset Formula $C,C^{\prime}\in M$
\end_inset

 induces a global section in 
\begin_inset Formula $\mathscr{S}\left(X\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The fact that 
\begin_inset Formula $\mathscr{S}$
\end_inset

 is a subpresheaf of 
\begin_inset Formula $\mathscr{E}$
\end_inset

 indicates that the possible assignments for experiments defined by 
\begin_inset Formula $\mathscr{S}$
\end_inset

 are indeed assignments to the set of outcomes.
 Condition 1) ensures that every context has at least one satisfying assignment.
 2) ensures that an assignment within a context can be restricted to a sub-assig
nment.
 We only assume that this is possible within a context, because a context
 consists of a collection of measurements which can be performed simultaneously
 without affecting their outcomes.
 Therefore, an assignment to a collection 
\begin_inset Formula $U\subset C$
\end_inset

 can always be extended to an assignment of outcomes on 
\begin_inset Formula $C$
\end_inset

 by performing the rest of the measurements.
 3) insists that every assignment to contexts which agrees on their overlap
 can be extended to a global section.
 In other words, any assignments to all measurements 
\begin_inset Formula $X$
\end_inset

 which agree on each context is assumed to actually provide a global section.
 This section must be unique, since 
\begin_inset Formula $\mathscr{S}\subset\mathscr{E}$
\end_inset

.
\end_layout

\begin_layout Standard
The question of logical contextuality is 
\begin_inset Quotes eld
\end_inset

When is a section over a context extendible to a global section?
\begin_inset Quotes erd
\end_inset

 By assumption 3), this is equivalent to asking whether or not there are
 compatible sections for every other context.
 We may also ask for strong logical contextuality: 
\begin_inset Quotes eld
\end_inset

Are there any global assignments?
\begin_inset Quotes erd
\end_inset

 There are various results relating to these questions which rely on cohomologic
al invariants, but there appears to be a much simpler solution.
\end_layout

\begin_layout Definition
Given a measurement scenario 
\begin_inset Formula $\left\langle X,M,O\right\rangle $
\end_inset

 and a possibilistic empirical model for that measurement scenario, define
 the graph 
\begin_inset Formula $\mathscr{G}$
\end_inset

 to have vertices 
\begin_inset Formula $\left\{ s_{C}\in\mathscr{S}\left(C\right)\right\} _{C\in M}$
\end_inset

.
 Let 
\begin_inset Formula $a\sim b$
\end_inset

 when 
\begin_inset Formula $\forall C$
\end_inset

, 
\begin_inset Formula $a\mid_{C}=b\mid_{C}$
\end_inset

 (in other words, the sections are compatible.)
\end_layout

\begin_layout Definition
Each vertex of 
\begin_inset Formula $\mathscr{G}$
\end_inset

 is defined to provide a an assignment of outcomes to a particular context.
 Each of the vertices in our orthogonality graph, 
\begin_inset Formula $G$
\end_inset

, also provides an assignment of outcomes to a particular context.
 The vertices of 
\begin_inset Formula $\mathscr{G}$
\end_inset

 are adjacent according to compatibility conditions, whereas the vertices
 in our graph are adjacent according to incompatibility conditions.
 Due to the technicality described in the previous section, it is possible
 that there will be more orthogonality conditions than incompatibility condition
s, but by adding additional measurements we can always ensure that the two
 agree.
 Thus, if our measurement scenario is provided with explicit measurements
 
\begin_inset Formula $X$
\end_inset

 in terms of Hermitian matrices, we can calculate 
\begin_inset Formula $G$
\end_inset

 and conclude that 
\begin_inset Formula $\overline{G}=\mathscr{G}$
\end_inset

.
 If our measurement scenario is not given according to explicit measurements,
 we have still defined 
\begin_inset Formula $\mathscr{G}$
\end_inset

.
\end_layout

\begin_layout Proposition
Our empirical model will be strongly contextual iff 
\begin_inset Formula $\omega\left(\mathscr{G}\right)\neq\left|M\right|$
\end_inset

.
 
\end_layout

\begin_layout Proof
Since no two sections over a particular context 
\begin_inset Formula $C$
\end_inset

 are compatible, 
\begin_inset Formula $\omega\left(\mathscr{G}\right)\leq\left|M\right|$
\end_inset

.
 Suppose that we have a global section, 
\begin_inset Formula $S$
\end_inset

.
 Then 
\begin_inset Formula $\forall C$
\end_inset

, 
\begin_inset Formula $S\mid_{C}$
\end_inset

 is a section over 
\begin_inset Formula $C$
\end_inset

, and thus corresponds to a vertex of 
\begin_inset Formula $\mathscr{G}$
\end_inset

.
 Moreover, for any two contexts 
\begin_inset Formula $C,C^{\prime}$
\end_inset

, we must have 
\begin_inset Formula $\left(S\mid_{C}\right)\mid_{C^{\prime}}=\left(S\mid_{C^{\prime}}\right)\mid_{C}=S\mid_{C\cap C^{\prime}}$
\end_inset

, which means that the corresponding vertices are adjacent.
 Hence, 
\begin_inset Formula $S$
\end_inset

 induces a clique of size 
\begin_inset Formula $M$
\end_inset

, and 
\begin_inset Formula $\omega\left(\mathscr{G}\right)=\left|M\right|$
\end_inset

.
\end_layout

\begin_layout Proof
Conversely, if 
\begin_inset Formula $\omega\left(\mathscr{G}\right)=\left|M\right|$
\end_inset

, then we have a clique of size 
\begin_inset Formula $\left|M\right|$
\end_inset

.
 The vertices of this clique correspond to sections for each context, and
 since every pair of vertices in the clique is adjacent, every pair of sections
 is compatible.
 Due to assumption 3) of empirical models, there is a global section.
\end_layout

\begin_layout Standard
Similarly, we can detect logical contextuality with respect to a section
 by determining whether or not the vertex in 
\begin_inset Formula $\mathscr{G}$
\end_inset

 associated with that section is part of a clique of size 
\begin_inset Formula $\left|M\right|$
\end_inset

.
\end_layout

\begin_layout Section*
State-Independent Contextuality
\end_layout

\begin_layout Standard
The Peres-Mermin square does not have any satisfying assignments.
 Thus, its classical set trivially obeys any Bell Inequality.
 A weaker form of state-independent contextuality (henceforth state-independent
 contextuality) occurs when a single Bell Inequality separates all quantum
 states from all classical ones.
 This stands in contrast to logical contextuality.
 Since the quantum set and classical sets are convex bodies, there is a
 separating hyperplane between the two.
 This hyperplane corresponds to a Bell Inequality.
 Thus, we do not gain a more general notion of state-independent contextuality
 by allowing the inequality to vary with the state.
\end_layout

\begin_layout Standard
This weaker form of contextuality can be recognized.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset citation
LatexCommand cite
key "cab15"

\end_inset

 State Independent Contextuality is equivalent to: We can assign non-negative
 numbers 
\begin_inset Formula $w=\left(w_{1},w_{2},\dots\right)$
\end_inset

 to the vertices of our orthogonality graph, such that
\begin_inset Formula 
\[
\sum_{j\in\alpha}w_{j}<1\mbox{ for all \ensuremath{\alpha} and }\sum_{v}w_{v}\Pi_{v}\geq I
\]

\end_inset


\end_layout

\begin_layout Theorem
where 
\begin_inset Formula $\Pi_{i}$
\end_inset

 is the projector associated with the vertex 
\begin_inset Formula $i$
\end_inset

.
 
\begin_inset Formula $\alpha$
\end_inset

 runs over the set of independent sets which meet every context.
 If this is the case, the functional in our inequality is given by the weighting
s 
\begin_inset Formula $w_{i}$
\end_inset

.
\end_layout

\begin_layout Proof
Suppose we have such an assignment of the weights.
 Then any classical assignment of outcomes corresponds to an independent
 set, where the sum of weights is strictly bounded by 
\begin_inset Formula $1$
\end_inset

 by fiat.
 For any state vector 
\begin_inset Formula $\psi$
\end_inset

, the weighted sum of probabilities is lower bounded 
\begin_inset Formula 
\[
\sum_{v}w_{v}\left\langle \psi\right|\Pi_{v}\left|\psi\right\rangle =\left\langle \psi\right|\left(\sum_{v}w_{v}\Pi_{v}\right)\left|\psi\right\rangle \geq\left\langle \psi\right|I\left|\psi\right\rangle =1
\]

\end_inset


\end_layout

\begin_layout Proof
Thus, we have state-independent contextuality.
\end_layout

\begin_layout Proof
Conversely, suppose we have some functional which separates all classical
 states from all quantum states.
 Such a functional corresponds to a corresponds to a weighting of the vertices
 
\begin_inset Formula $w_{i}$
\end_inset

 of non-negative numbers such that for some 
\begin_inset Formula $A\in\mathbb{R}$
\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\sum_{j\in\alpha}w_{j}<A\mbox{ and }\sum_{v}w_{v}\left\langle \psi\right|\Pi_{v}\left|\psi\right\rangle \geq A
\]

\end_inset


\end_layout

\begin_layout Proof
If we rescale our weights, we can set 
\begin_inset Formula $A=1$
\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\sum_{j\in\alpha}w_{j}<1\mbox{ and }\sum_{v}w_{v}\left\langle \psi\right|\Pi_{v}\left|\psi\right\rangle \geq1=\left\langle \psi\right|I\left|\psi\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Proof
Now we re-arrange the second condition.
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
0\leq\sum_{v}w_{v}\left\langle \psi\right|\Pi_{v}\left|\psi\right\rangle -\left\langle \psi\right|I\left|\psi\right\rangle =\left\langle \psi\right|\left(\sum_{v}w_{v}\Pi_{v}-I\right)\left|\psi\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Proof
and holds occurs for all 
\begin_inset Formula $\psi$
\end_inset

, in particular for any eigenvectors of 
\begin_inset Formula $\left(\sum_{v}w_{v}\Pi_{v}-I\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $\left(\sum_{v}w_{v}\Pi_{v}-I\right)$
\end_inset

 is positive semidefinite and we have 
\begin_inset Formula $\sum_{v}w_{v}\Pi_{v}\geq I$
\end_inset

.
\end_layout

\begin_layout Standard
It is claimed
\begin_inset CommandInset citation
LatexCommand cite
key "cab15"

\end_inset

 that this necessary and sufficient condition can be verified by a semidefinite
 program.
 However, it's a little unsatisfying because it is relies on eigenvectors
 and is not fully combinatorial.
 There is a simple necessary combinatorial condition
\begin_inset CommandInset citation
LatexCommand cite
key "ram14"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "cab15"

\end_inset

.
\end_layout

\begin_layout Proposition
(!!)In order to be state-independent contextual, the underlying orthogonality
 graph must satisfy
\end_layout

\begin_layout Proposition
\begin_inset Formula 
\[
\omega^{\star}\left(G\right)>d
\]

\end_inset


\end_layout

\begin_layout Proposition
where 
\begin_inset Formula $\omega^{\star}\left(G\right)=\alpha^{\star}\left(\overline{G}\right)$
\end_inset

 is the fractional chromatic number and 
\begin_inset Formula $d$
\end_inset

 is the dimension.
\end_layout

\begin_layout Proof
If we have state-independent contextuality, then such contextuality must
 hold for the maximally mixed state, which assigns a probability of 
\begin_inset Formula $\frac{1}{d}$
\end_inset

 to every vector.
 Thus, we need this probability distribution to be outside of the classical
 set before we can hope to have state-independent contextuality.
 The condition above is a clever way to check if this distribution is classical
 or not.
\end_layout

\begin_layout Proof
The antiblocker theorem (Theorem 25) between 
\begin_inset Formula $VP\left(G\right)$
\end_inset

 and 
\begin_inset Formula $FVP\left(G\right)$
\end_inset

 states that a point 
\begin_inset Formula $x\in VP\left(G\right)$
\end_inset

 iff 
\begin_inset Formula $\forall y\in FVP\left(\overline{G}\right)$
\end_inset

, 
\begin_inset Formula $x^{T}y\leq1$
\end_inset

, thus, we would like to calculate 
\begin_inset Formula $\max_{y}x^{T}y$
\end_inset

, where 
\begin_inset Formula $y$
\end_inset

 ranges over 
\begin_inset Formula $FVP\left(\overline{G}\right)=VP\left(G\right)$
\end_inset

.
 This exactly defines 
\begin_inset Formula $\alpha^{\star}\left(\overline{G};x\right)=\omega^{\star}\left(G;x\right)$
\end_inset

, where 
\begin_inset Formula $x$
\end_inset

 is now the weighting.
 In particular, we can choose 
\begin_inset Formula $x$
\end_inset

 to be the constant weighting where each vertex is weighted with 
\begin_inset Formula $\frac{1}{d}$
\end_inset

.
 Since everything is linear, we can conclude that 
\begin_inset Formula $\frac{1}{d}\omega^{\star}\left(G\right)\leq1$
\end_inset

 iff the constant probability 
\begin_inset Formula $\frac{1}{d}$
\end_inset

 is in 
\begin_inset Formula $VP\left(G\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The proof above contains a big hole: it shows that the maximally mixed state
 provides a probability distribution which is outside of 
\begin_inset Formula $VP\left(G\right)$
\end_inset

 iff 
\begin_inset Formula $\omega^{\star}\left(G\right)>d$
\end_inset

.
 However, 
\begin_inset Formula $VP\left(G\right)$
\end_inset

 is defined as the convex hull of independent sets, not the convex hull
 of independent sets which meet every context.
 Thus, it is possible that the maximally mixed state might be part of 
\begin_inset Formula $VP\left(G\right)$
\end_inset

, yet outside the set of classical assignments.
 It's plausible that when the maximally mixed state is part of 
\begin_inset Formula $VP\left(G\right)$
\end_inset

, the quantum set and the classical set will necessarily interesct at some
 point, but this needs proof.
 Before any kind of proof, it is clear that our framework needs refinement.
 Since we have been requiring that our probabilities (both classical and
 quantum) sum to exactly 
\begin_inset Formula $1$
\end_inset

 in each context (rather than being bounded by 
\begin_inset Formula $1$
\end_inset

), we need tools to explicitly represent the contexts which are considered.
 Such tools are provided in the hypergraph approach
\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset

.
\end_layout

\begin_layout Section*
The Hypergraph Approach
\end_layout

\begin_layout Standard
Our orthogonality graphs only keep track of outcomes and compatibilities
 of those outcomes.
 There is no mention of which outcomes actually form a context and need
 to have probabilities summing to 
\begin_inset Formula $1$
\end_inset

.
 In most of our cases, the 'outcomes' can be considered to be 
\begin_inset Formula $1$
\end_inset

-dimensional projectors corresponding to the simultaneous eigenvectors of
 the collections of commuting observables.
\end_layout

\begin_layout Standard
An alternative approach
\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset

 makes explicit use of the fact that probabilities sum to exactly 
\begin_inset Formula $1$
\end_inset

.
 In this hypergraph approach, our measurements correspond to hyperedges
 whose vertices are outcomes.
 
\end_layout

\begin_layout Definition
A hypergraph consists of a finite set of vertices, 
\begin_inset Formula $V$
\end_inset

 (which correspond to outcomes), together with a collection of hyperedges
 
\begin_inset Formula $E$
\end_inset

, 
\begin_inset Formula $H\subset2^{V}$
\end_inset

, which correspond to our contexts.
 We assume that these hyperedges form an antichain: for any two 
\begin_inset Formula $h_{1},h_{2}\in H$
\end_inset

, we have 
\begin_inset Formula $h_{1}\not\subset h_{2}$
\end_inset

 and that 
\begin_inset Formula $\bigcup_{e\in E}=V$
\end_inset

.
 (These are also called clutters)
\end_layout

\begin_layout Standard
If our measurements consist of 
\begin_inset Formula $1$
\end_inset

-d projectors, then these hyperedges will each contain 
\begin_inset Formula $d$
\end_inset

 vertices, where 
\begin_inset Formula $d$
\end_inset

 is the dimension of the space.
 However, we do not assume any particular structure for the measurements
 so that the framework is general.
\end_layout

\begin_layout Standard
First, observe that a graph can be recovered from any hypergraph by defining
 
\end_layout

\begin_layout Definition
Given a hypergraph 
\begin_inset Formula $H$
\end_inset

, 
\begin_inset Formula $Or\left(H\right)$
\end_inset

 is the graph with vertices 
\begin_inset Formula $V\left(H\right)$
\end_inset

 and edges 
\begin_inset Formula $w\sim v$
\end_inset

 iff 
\begin_inset Formula $w,v\subset e\in E\left(H\right)$
\end_inset

 for some hyperedge 
\begin_inset Formula $e$
\end_inset

.
 
\end_layout

\begin_layout Standard
The graph defined in this way is an orthogonality graph is our usual sense:
 if two vertices are contained within the same hyperedge, then this means
 that these two vertices correspond to different outcomes for the same context.
 If we perform a quantum measurement corresponding to this hyperedge, then
 the outcomes of that measurement will correspond to the eigenspaces of
 our measurment, and different outcomes will be in orthogonal subspaces.
 If we are given specific observables to serve as the hyperedges then we
 might find more orthogonalities.
 The type of orthogonality that comes automatically from disagreeing tests
 is called 
\begin_inset Quotes eld
\end_inset

local orthogonality,
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Just as we have defined 
\begin_inset Formula $FVP\left(G\right)$
\end_inset

, 
\begin_inset Formula $TH\left(G\right)$
\end_inset

, and 
\begin_inset Formula $VP\left(G\right)$
\end_inset

 for orthogonality graphs which capture the general, quantum, and classical
 probability distributions on outcomes, we can also define three classes
 of probability distributions for hypergraphs, along with charaterizations
 of these bodies which follow from the Antiblocker property.
 
\end_layout

\begin_layout Standard
Our original use of the convex bodies 
\begin_inset Formula $FVP,TH,VP$
\end_inset

 was to derive Bell Inequalities.
 The graphs given as input were actually subgraphs of the graph of the system
 we were interested in.
 Thus, we allowed that the probabilities be subnormalized, because the vertices
 which weren't part of our subgraph could hide some of the probability.
 In the hypergraph approach, we require that each measurement be normalized.
\end_layout

\begin_layout Definition
A probabilistic model on a hypergraph 
\begin_inset Formula $H$
\end_inset

 consists of an assignment 
\begin_inset Formula $p:V\left(H\right)\to\left[0,1\right]$
\end_inset

 such that 
\begin_inset Formula $\sum_{v\in e}p\left(v\right)=1$
\end_inset

.
\end_layout

\begin_layout Standard
One tradeoff is that there are now hypergraphs which do not admit any probabilis
tic models.
\end_layout

\begin_layout Proposition
For a hypergraph 
\begin_inset Formula $H$
\end_inset

, probabilistic model 
\begin_inset Formula $p$
\end_inset

 is extremal exactly when there is a subhypergraph 
\begin_inset Formula $W\subset H$
\end_inset

 such that 
\begin_inset Formula $W$
\end_inset

 admits a unique probabilistic model 
\begin_inset Formula $p_{W}$
\end_inset

, which extends to 
\begin_inset Formula $p$
\end_inset

 by setting 
\begin_inset Formula $p\left(v\right)=0$
\end_inset

 when 
\begin_inset Formula $v\not\in W$
\end_inset

.
\end_layout

\begin_layout Proof
By induction.
 The base case is when 
\begin_inset Formula $H$
\end_inset

 itself has a unique probabilistic model, and in this case there is nothing
 to prove.
\end_layout

\begin_layout Proof
Otherwise, probabilistic models, by definition, satisfy
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
p\left(v\right)\geq0,\mbox{ and }\sum_{v\in e}p\left(v\right)=1\mbox{ }\forall e\in E\left(H\right)
\]

\end_inset


\end_layout

\begin_layout Proof
This is a polytope, so extreme points will be extreme points of facets.
 A facet will consist of those probability distributions where one of the
 constraining inequalities is met.
 Hence, there is some vertex 
\begin_inset Formula $v$
\end_inset

 such that 
\begin_inset Formula $p\left(v\right)=0$
\end_inset

.
 Define 
\begin_inset Formula $W^{1}=H-\left\{ v\right\} $
\end_inset

 and recurse.
\end_layout

\begin_layout Standard
We do not assume that these models obey the exclusivity principle.
 Their summations are only normalized over hyperedges, and 
\begin_inset Formula $Or\left(H\right)$
\end_inset

 may include cliques which do not correspond to edges.
 The omission of the exclusivity principle in
\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset

 was deliberate so as to allow two hypergraphs 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

 to be tensored and so that this tensor product would admit the tensor of
 the probabilistic models on each of 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

.
 The scheme for tensoring these two hypergraphs is complicated and is omitted
 here.
 The point of doing this is to explore non-locality, which we might revisit
 later.
\end_layout

\begin_layout Proposition
Given a hypergraph 
\begin_inset Formula $H$
\end_inset

, a probabilistic model 
\begin_inset Formula $p:V\left(H\right)\to\left[0,1\right]$
\end_inset

 satisfies the exclusivity principle iff 
\begin_inset Formula $\omega\left(Or\left(H\right);p\right)=1$
\end_inset

.
\end_layout

\begin_layout Proof
First, suppose that we have a probabilistic model, 
\begin_inset Formula $p$
\end_inset

 which satisfies the exclusivity principle.
 The exclusivity principle states 
\begin_inset Formula $\omega\left(Or\left(H\right);p\right)\leq1$
\end_inset

.
 Any hyperedge, 
\begin_inset Formula $e$
\end_inset

, will yield a clique in 
\begin_inset Formula $Or\left(H\right)$
\end_inset

 for which 
\begin_inset Formula $\sum_{v\in e}p\left(v\right)=1$
\end_inset

, so 
\begin_inset Formula $\omega\left(Or\left(H\right);p\right)=1$
\end_inset

.
\end_layout

\begin_layout Proof
Conversely, suppose that 
\begin_inset Formula $\omega\left(Or\left(H\right);p\right)=1$
\end_inset

.
 Then 
\begin_inset Formula $\omega\left(Or\left(H\right);p\right)\leq1$
\end_inset

, and the exclusivity principle is satisfied.
\end_layout

\begin_layout Standard
For us, satisfying the exclusivity principle defines what is meant by a
 general probabilistic model.
 The probabilities 
\begin_inset Formula $p$
\end_inset

 for which 
\begin_inset Formula $\omega\left(Or\left(H\right);p\right)$
\end_inset

 can be interpreted with the antiblocker property.
 These are the points such that scaling up by any amount would remove 
\begin_inset Formula $p$
\end_inset

 from 
\begin_inset Formula $FVP\left(Or\left(H\right)\right)$
\end_inset

.
 These are the points on the boundary of 
\begin_inset Formula $FVP\left(Or\left(H\right)\right)$
\end_inset

 and are also probabilistic models.
 It is not clear whether or not these 'general probabilistic models' in
 this sense are convex.
\end_layout

\begin_layout Definition
A probabilistic model is deterministic if 
\begin_inset Formula $p\left(v\right)\in\left\{ 0,1\right\} $
\end_inset

 for all 
\begin_inset Formula $v\in V\left(H\right)$
\end_inset

.
 A probabilistic model is classical if it is a convex combination of determinist
ic models.
\end_layout

\begin_layout Standard
In constrast to the deterministic models for graphs, our definition of probabili
stic models requires that the probabilities within every context sum to
 
\begin_inset Formula $1$
\end_inset

.
 This captures that the deterministic models are not only independent sets
 of 
\begin_inset Formula $Or\left(H\right)$
\end_inset

 but also that these independent sets must meet every clique.
\end_layout

\begin_layout Proposition
A probabilistic model 
\begin_inset Formula $p$
\end_inset

 is classical iff 
\begin_inset Formula $\omega^{\star}\left(Or\left(H\right);p\right)=1$
\end_inset

.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $p$
\end_inset

 is a probabilistic model, we have 
\begin_inset Formula $\omega^{\star}\left(Or\left(H\right);p\right)\geq\omega\left(Or\left(H\right);p\right)\geq1$
\end_inset

, since any hyperedge determines a clique whose sum of probabilities is
 
\begin_inset Formula $1$
\end_inset

.
 
\end_layout

\begin_layout Proof
Recall that the feasible set of 
\begin_inset Formula $\omega^{\star}\left(G\right)$
\end_inset

 is 
\begin_inset Formula $FVP\left(\overline{G}\right)$
\end_inset

.
 The antiblocker property says that for any graph 
\begin_inset Formula $G$
\end_inset

, 
\begin_inset Formula $FVP\left(G\right)=AB\left(VP\left(\overline{G}\right)\right)$
\end_inset

.
 If 
\begin_inset Formula $p\in VP\left(Or\left(H\right)\right)$
\end_inset

, so for any 
\begin_inset Formula $q\in FVP\left(\overline{Or\left(H\right)}\right)$
\end_inset

, 
\begin_inset Formula $p\cdot q\leq1$
\end_inset

.
 Since 
\begin_inset Formula $\omega^{\star}\left(Or\left(H\right),p\right)$
\end_inset

 is defined to be 
\begin_inset Formula $\max_{q\in FVP\left(\overline{Or\left(H\right)}\right)}q\cdot p$
\end_inset

, we have 
\begin_inset Formula $\omega^{\star}\left(Or\left(H\right),p\right)\leq1$
\end_inset

, so 
\begin_inset Formula $\omega^{\star}\left(Or\left(H\right),p\right)=1.$
\end_inset


\end_layout

\begin_layout Proof
Conversely, suppose that 
\begin_inset Formula $\omega^{\star}\left(Or\left(H\right);p\right)=1$
\end_inset

.
 Then, 
\begin_inset Formula $\forall q\in FVP\left(\overline{Or\left(H\right)}\right)$
\end_inset

, 
\begin_inset Formula $q\cdot p\leq1$
\end_inset

, so 
\begin_inset Formula $p\in AB\left(FVP\left(\overline{Or\left(H\right)}\right)\right)=VP\left(Or\left(H\right)\right)$
\end_inset

.
 In other words, 
\begin_inset Formula $p$
\end_inset

 can be written as a convex combination of independent sets.
 Each of these independent sets must be maximal.
 If 
\begin_inset Formula $p=x+p^{\prime}$
\end_inset

, where 
\begin_inset Formula $x$
\end_inset

 is an independent set which does not meet an edge 
\begin_inset Formula $e_{x}$
\end_inset

 and 
\begin_inset Formula $p^{\prime}$
\end_inset

 consists of the remaining terms of a decomposition of 
\begin_inset Formula $p$
\end_inset

 into independent sets, then 
\begin_inset Formula $\sum_{v\in e_{x}}p\left(v\right)=\sum_{v\in e_{x}}p^{\prime}\left(v\right)<1$
\end_inset

, which contradicts that 
\begin_inset Formula $p$
\end_inset

 was a probabilistic model.
\end_layout

\begin_layout Standard
The condition described by the previous proposition indicates that the set
 of classical probability models consists of a particular facet of 
\begin_inset Formula $VP\left(G\right)$
\end_inset

, bounded by the vertices which are independent sets which meet every context.
\end_layout

\begin_layout Standard
The case of 
\begin_inset Formula $TH\left(G\right)$
\end_inset

 is interesting.
 It would be natural to define the quantum models by analogy with the previous
 two propositions:
\end_layout

\begin_layout Definition
A probabilistic model 
\begin_inset Formula $p\in Q_{1}\left(H\right)$
\end_inset

 if 
\begin_inset Formula $\vartheta\left(\left(\overline{Or\left(H\right)},p\right)\right)=1$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset

.
\end_layout

\begin_layout Standard
However, we will se that 
\begin_inset Formula $Q_{1}\left(H\right)$
\end_inset

 is larger than the quantum set:
\end_layout

\begin_layout Definition
The quantum set, 
\begin_inset Formula $Q$
\end_inset

, is the collection of probabilities 
\begin_inset Formula $p:V\left(H\right)\to\left[0,1\right]$
\end_inset

 such that there exists a Hilbert space 
\begin_inset Formula $\mathscr{H}$
\end_inset

, a quantum state 
\begin_inset Formula $\rho\in\mathscr{B}_{+,1}\left(\mathscr{H}\right)$
\end_inset

 and a projection operator 
\begin_inset Formula $P_{v}\in\mathscr{B}\left(\mathscr{H}\right)$
\end_inset

 associated to every 
\begin_inset Formula $v\in V$
\end_inset

 which are projective measurments:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\[
\sum_{v\in e}P_{v}=I_{\mathscr{H}}
\]

\end_inset


\end_layout

\begin_layout Definition
and reproduce the given probabilities 
\begin_inset Formula $p\left(v\right)$
\end_inset

 according to 
\begin_inset Formula $p\left(v\right)=Tr\left(\rho P_{v}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
There are several distinctions between 
\begin_inset Formula $Q_{1}$
\end_inset

 and the quantum set.
 In 
\begin_inset Formula $Q_{1}$
\end_inset

, we require that our projectors be 
\begin_inset Formula $1$
\end_inset

-d, and we also require that the Hilbert Space be finite dimensional.
 These are both conditions which might suggest that 
\begin_inset Formula $Q_{1}\subset Q$
\end_inset

, but in fact 
\begin_inset Formula $Q\subset Q_{1}$
\end_inset

.
 In the quantum set, we require that the projectors sum to 
\begin_inset Formula $I_{\mathscr{H}}$
\end_inset

.
 For 
\begin_inset Formula $p\in Q_{1}$
\end_inset

, we have 
\begin_inset Formula $\sum_{v\in e}p\left(v\right)=1$
\end_inset

 (since 
\begin_inset Formula $p$
\end_inset

 is a probabilistic model) for some particular state 
\begin_inset Formula $\psi$
\end_inset

 (the handle), but the projectors will not sum to the identity unless the
 dimension of the Orthonormal Representation is the size of each context.
 It might be possible to extend the 1-d projectors to higher-dimensional
 projectors which agree with the original projectors for 
\begin_inset Formula $\psi$
\end_inset

, but these projections will need to be compatible with one another.
 Thus, the quantum set has a more stringent requirement.
\end_layout

\begin_layout Standard
Unlike the set of classical and general probability sets, the quantum set
 cannot be defined by the underlying orthogonality graph.
\end_layout

\begin_layout Proposition
There exists a hypergraph 
\begin_inset Formula $H$
\end_inset

 for which 
\begin_inset Formula $Q\left(H\right)\neq Q_{1}\left(H\right)$
\end_inset


\end_layout

\begin_layout Proof
(Not really a proof) 
\begin_inset CommandInset citation
LatexCommand cite
key "nav14"

\end_inset

 We can characterize the quantum set of 
\begin_inset Formula $B_{2,2,2}$
\end_inset

 by the angle between the two choices of measurement for one of the parties.
 There is a particular functional which separates 
\begin_inset Formula $Q_{1}$
\end_inset

 from 
\begin_inset Formula $Q$
\end_inset

 by direct computation.
\end_layout

\begin_layout Standard
It would be nice to have a good explanation for why the 
\begin_inset Quotes eld
\end_inset

almost quantum set
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $Q_{1}$
\end_inset

 differs from 
\begin_inset Formula $Q$
\end_inset

.
 However, according to 
\begin_inset CommandInset citation
LatexCommand cite
key "nav14"

\end_inset

 
\begin_inset Formula $Q_{1}$
\end_inset

 satisfies most information principles which have been conjectured to characteri
ze 
\begin_inset Formula $Q$
\end_inset

.
 This makes it difficult to find a high-level reason for the discrepancy
 between 
\begin_inset Formula $Q_{1}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

.
 It would be nice to have some sort of general recipe for generating probabiliti
es which lie in 
\begin_inset Formula $Q_{1}-Q$
\end_inset

.
\end_layout

\begin_layout Corollary
There are two hypergraphs, 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

 with the same underlying orthogonality graph, but different quantum sets.
 That is, the quantum set is not determined by the underlying orthogonality
 graph.
\end_layout

\begin_layout Corollary
TODO: collect the proof from 
\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset

.
\end_layout

\begin_layout Section*
The NPA Hierarchy
\end_layout

\begin_layout Standard
The Quantum Set is mysterious- its definition does not suggest an algorithm
 for determining whether or not a probabilistic model is quantum.
 However, it is simple to certify that a probabilistic model is quantum
 by providing the projectors which give rise to the desired probabilities.
 While it is conjectured that membership in the quantum is not decidable
\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset

, there is a collection of semidefinite programs which completely determines
 it
\begin_inset CommandInset citation
LatexCommand cite
key "nav08"

\end_inset

.
\end_layout

\begin_layout Standard
Suppose that we have a probabilistic model which is actually quantum.
 According to Definition 43, there are projectors which can be associated
 to each vertex, such that projectors 
\begin_inset Formula $P_{v}$
\end_inset

 in any hyperedge are mutually orthogonal and sum to 
\begin_inset Formula $I$
\end_inset

, and a state 
\begin_inset Formula $\left|\psi\right\rangle $
\end_inset

 which produces the desired probabilities.
 Consider the vectors of the form 
\begin_inset Formula $\left\{ \left(\prod_{i=1}^{n}P_{v_{i}}\right)\left|\psi\right\rangle \cup\left|\psi\right\rangle \right\} $
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is finite.
 We can form a semidefinite matrix, 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 by taking the Gram matrix for these vectors.
 By its construction 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 is positive semidefinite.
 Also, if the Hilbert Space has dimension 
\begin_inset Formula $N<n$
\end_inset

, then no more than 
\begin_inset Formula $N$
\end_inset

 of our vectors are linearly independent.
 Hence, the rank of 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 will be at most 
\begin_inset Formula $N$
\end_inset

.
\end_layout

\begin_layout Standard
Any edge of the hypergraph must give its vectors a decomposition of 
\begin_inset Formula $I$
\end_inset

 into projectors.
 This enforces constraints on our projectors 
\begin_inset Formula $P_{v}$
\end_inset

, which, in turn, are constraints on the entries of 
\begin_inset Formula $\Gamma^{n}$
\end_inset

.
 If 
\begin_inset Formula $v_{1}\sim v_{2}$
\end_inset

, then they must correspond to orthogonal projectors.
 Hence, for any product of projectors 
\begin_inset Formula $S$
\end_inset

, we will have 
\begin_inset Formula $P_{v_{1}}S\left|\psi\right\rangle \perp P_{v_{2}}S\left|\psi\right\rangle $
\end_inset

.
\end_layout

\begin_layout Standard
Any the projectors associated with any edge in the hypergraph must sum to
 
\begin_inset Formula $I$
\end_inset

.
 This translates to the fact that for any product of projector 
\begin_inset Formula $S$
\end_inset

, we have 
\begin_inset Formula $\sum_{v\in E}P_{v}P_{S}\left|\psi\right\rangle =P_{S}\left|\psi\right\rangle $
\end_inset

.
\end_layout

\begin_layout Standard
Putting our observations in terms of 
\begin_inset Formula $\Gamma^{n}$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Gamma^{n}\succeq0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Gamma_{\emptyset,\emptyset}^{n}=1$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left(\forall s,w\right)\sum_{v\in E}\Gamma_{sv,w}^{n}=\Gamma_{s,w}^{n}$
\end_inset


\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $v=v_{1}\dots v_{k}$
\end_inset

 and 
\begin_inset Formula $w=w_{1}\dots w_{m}$
\end_inset

, with 
\begin_inset Formula $v_{k}\sim w_{m}$
\end_inset

, then 
\begin_inset Formula $\Gamma_{v,w}^{n}=0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Gamma_{\emptyset,v}^{n}=Pr\left(v\right)$
\end_inset


\end_layout

\begin_layout Standard
Thus, if our probabilistic model, 
\begin_inset Formula $p$
\end_inset

, is actually quantum, then for any 
\begin_inset Formula $n$
\end_inset

, we can find a certificate, 
\begin_inset Formula $\Gamma^{n}$
\end_inset

, which satisfies properties 1-5.
 The NPA hierarchy at level 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $Q_{n}$
\end_inset

, is the set of probabilistic models such that there exists a certificate
 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 which satisfies 1-5.
 We will see that the notation agrees with definition 
\begin_inset Formula $42$
\end_inset

, and that 
\begin_inset Formula $Q=Q_{\infty}$
\end_inset

.
 First, we note that the certificates 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 really do form a hierarchy.
\end_layout

\begin_layout Proposition
Given a hypergraph 
\begin_inset Formula $H$
\end_inset

, 
\begin_inset Formula $Q\left(H\right)\subset\dots\subset Q_{n}\left(H\right)\subset\dots\subset Q_{1}\left(H\right)$
\end_inset

.
\end_layout

\begin_layout Proof
We have already argued that 
\begin_inset Formula $\forall n$
\end_inset

, 
\begin_inset Formula $Q\left(H\right)\subset Q_{n}\left(H\right)$
\end_inset

, since if our model is really quantum, then we can construct 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 as the Gram matrix of vectors of the form 
\begin_inset Formula $\left\{ \left(\prod_{i=1}^{n}P_{v_{i}}\right)\left|\psi\right\rangle \cup\left|\psi\right\rangle \right\} $
\end_inset

.
 Suppose that we have a probabilistic model 
\begin_inset Formula $p\in Q_{n}\left(H\right)$
\end_inset

.
 Then we have a certificate 
\begin_inset Formula $\overline{\Gamma^{n}}$
\end_inset

 which certifies 
\begin_inset Formula $p$
\end_inset

.
 By restricting this matrix to entries involving only 
\begin_inset Formula $n-1$
\end_inset

 projectors, we obtain a 
\begin_inset Formula $Q_{n-1}$
\end_inset

 certificate for 
\begin_inset Formula $\Gamma^{n-1}$
\end_inset

.
\end_layout

\begin_layout Theorem
The NPA hierarchy exactly characterizes the quantum set.
 
\begin_inset Formula $\bigcap_{n=1}^{\infty}Q_{n}\left(H\right)=Q\left(H\right)$
\end_inset

.
\end_layout

\begin_layout Proof
[sketch] The previous proposition implies that 
\begin_inset Formula $Q\left(H\right)\subset\bigcap_{n=1}^{\infty}Q_{n}\left(H\right)$
\end_inset

.
 To show the other direction, suppose that 
\begin_inset Formula $p$
\end_inset

 is a probabilistic model in 
\begin_inset Formula $\bigcap_{n=1}^{\infty}Q_{n}\left(H\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $p$
\end_inset

 posesses certificates 
\begin_inset Formula $\overline{\Gamma^{n}}$
\end_inset

 for each 
\begin_inset Formula $n$
\end_inset

.
 The entries of each certificate are bounded, since they come from inner
 products of projections of unit vectors.
 Each certificate can be viewed as a vector in the 
\begin_inset Formula $l_{\infty}$
\end_inset

 norm, if we complete the non-existant entries to 
\begin_inset Formula $0$
\end_inset

.
 The Banach-Alaoglu theorem states that the unit ball is compact in the
 weak-
\begin_inset Formula $\star$
\end_inset

 topology, so our sequence of certificates has a convergent subsequence.
 A convergent subsequence in the weak-
\begin_inset Formula $\star$
\end_inset

 topology will converge when any functional 
\begin_inset Formula $\rho:v\to\mathbb{R}$
\end_inset

 is applied.
 In particular, if 
\begin_inset Formula $\rho$
\end_inset

 is returns a single component, we see that convergence in the weak-
\begin_inset Formula $\star$
\end_inset

 topology implies pointwise convergence.
 Hence, we have an infinite certificate 
\begin_inset Formula $\Gamma^{\infty}$
\end_inset

 which satisfies 2-5, and any submatrix formed by restriction is positive
 semidefinite.
 (!) By taking limits, we can see that 
\begin_inset Formula $\Gamma^{\infty}$
\end_inset

 itself is positive semidefinite.
\end_layout

\begin_layout Proof
If the behavior of 
\begin_inset Formula $p$
\end_inset

 is quantum, then the vertices of 
\begin_inset Formula $H$
\end_inset

 form a 
\begin_inset Formula $C^{\star}$
\end_inset

-algebra.
 The infinite certificate, 
\begin_inset Formula $\Gamma^{\infty}$
\end_inset

 defines a state on this 
\begin_inset Formula $C^{\star}$
\end_inset

 algebra, and the GNS construction provides a way of converting the 
\begin_inset Formula $C^{\star}$
\end_inset

-algebra into operators on a Hilbert Space, and the state into a vector
 in this Hilbert Space.
\end_layout

\begin_layout Proof
In order to use the GNS construction, we must argue that 
\begin_inset Formula $\Gamma^{\infty}$
\end_inset

 really does define a state on the 
\begin_inset Formula $C^{\star}$
\end_inset

-algebra.
 Suppose that 
\begin_inset Formula $X$
\end_inset

 is in this 
\begin_inset Formula $C^{\star}$
\end_inset

-algebra, so that 
\begin_inset Formula $X$
\end_inset

 can be written as a linear combination of products of vertices in the hypergrap
h.
 The state, 
\begin_inset Formula $\psi_{\Gamma^{\infty}}$
\end_inset

, must be shown to be a positive linear functional of norm 
\begin_inset Formula $1$
\end_inset

.
 We define 
\begin_inset Formula $\psi_{\Gamma^{\infty}}\left(P_{v_{1}}P_{v_{2}}\dots P_{v_{n}}\right)$
\end_inset

 by looking up 
\begin_inset Formula $\Gamma_{\emptyset,P_{v_{1}}\dots P_{v_{n}}}^{\infty}$
\end_inset

 and we extend this by linearity.
 The norm is 
\begin_inset Formula $1$
\end_inset

, 
\begin_inset Formula $\psi_{\Gamma^{\infty}}\left(I\right)=\Gamma_{\emptyset,\emptyset}^{\infty}=1$
\end_inset

, by 2).
 To see that 
\begin_inset Formula $\psi_{\Gamma^{\infty}}$
\end_inset

 is positive, we must show that 
\begin_inset Formula $\psi_{\Gamma^{\infty}}\left(X^{\star}X\right)\geq0$
\end_inset

.
 This follows from the fact that 
\begin_inset Formula $\Gamma^{\infty}$
\end_inset

 is positive semidefinite, since if we write 
\begin_inset Formula $X$
\end_inset

 as a linear combination of the vertices (projectors, or generators of our
 
\begin_inset Formula $C^{\star}$
\end_inset

-algebra), 
\begin_inset Formula $X=\sum_{i=1}^{n}\left(x_{k}\prod_{i}v_{i\in I_{k}}\right)$
\end_inset

, then 
\begin_inset Formula $\psi_{\Gamma^{\infty}}\left(X^{\star}X\right)=\left\langle x\right|\Gamma^{\infty}\left|x\right\rangle $
\end_inset

, where 
\begin_inset Formula $\left|x\right\rangle =\sum_{k=1}^{n}x_{k}\left|\prod_{i\in I_{k}}v_{i}\right\rangle $
\end_inset


\end_layout

\begin_layout Standard
The certificates 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 are not a semidefinite program, since they do not involve an optimization.
 They are convex bodies defined by a semidefinite condition, so they are
 feasible sets of a semidefinite program.
 Checking whether a particular probabilistic model lies in the 
\begin_inset Formula $n^{th}$
\end_inset

 level of the hierarchy amounts to checking whether or not this feasible
 set is empty, which can be achieved by the semidefinite program:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\max\left(\lambda\right)
\]

\end_inset


\end_layout

\begin_layout Standard
subject to
\begin_inset Formula 
\[
\Gamma^{n}-\lambda I\succeq0
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 is now assumed to satisfy 2-5.
 If it can satisfy 1) as well, then 
\begin_inset Formula $\lambda=0$
\end_inset

 is a solution.
 Otherwise, the feasible set is empty.
\end_layout

\begin_layout Lemma
The rank of a gram matrix is the dimension of the span of the vectors in
 its decomposition.
\end_layout

\begin_layout Proof
We use the fact that the rank of a gram matrix is the dimension of the space
 spanned by the vectors of this gram matrix.
 First, suppose that the vectors are all linearly independent.
 Then the Cholesky decomposition 
\begin_inset Formula $\Gamma=L^{T}L$
\end_inset

 shows that the 
\begin_inset Formula $\Gamma$
\end_inset

 is non-singular when 
\begin_inset Formula $L$
\end_inset

 is non-singular, and 
\begin_inset Formula $L$
\end_inset

 will be non-singular if its vectors are all linearly independent.
 
\end_layout

\begin_layout Proof
Now, suppose that our vectors consist of 
\begin_inset Formula $\left\{ a_{1},\dots,a_{n},b_{1},\dots,b_{m}\right\} $
\end_inset

, where the 
\begin_inset Formula $a_{i}$
\end_inset

's are all linearly independent and the 
\begin_inset Formula $b_{i}$
\end_inset

's may be written as a linear combination of the 
\begin_inset Formula $a_{i}$
\end_inset

's.
 The first row of the matrix will be 
\begin_inset Formula $\left(a_{1}\cdot a_{1},a_{1}\cdot a_{2},\dots a_{1}\cdot a_{n},a_{1}\cdot b_{1},\dots a_{1}\cdot b_{m}\right)$
\end_inset

, and the 
\begin_inset Formula $n+1^{th}$
\end_inset

 row will be 
\begin_inset Formula $\left(b_{1}\cdot a_{1},b_{1}\cdot a_{2},b_{1},\dots b_{1}\cdot a_{n},b_{1}\cdot b_{1}\dots b_{1}\cdot b_{m}\right)$
\end_inset

.
 By assumption, we can write 
\begin_inset Formula $b_{1}$
\end_inset

 as a linear combination of the 
\begin_inset Formula $a_{i}$
\end_inset

's, and thus we can write the 
\begin_inset Formula $n+1^{th}$
\end_inset

 row as a linear combination of the previous rows.
 Thus, the rows greater than 
\begin_inset Formula $n$
\end_inset

 do not contribute to the rank of 
\begin_inset Formula $\Gamma$
\end_inset

.
\end_layout

\begin_layout Lemma
Suppose we have a finite collection of projectors 
\begin_inset Formula $\left\{ P_{i}\right\} _{i=1}^{n}$
\end_inset

 such that 
\begin_inset Formula $\sum_{i=1}^{n}P_{i}=I$
\end_inset

.
 Then for 
\begin_inset Formula $i\neq j$
\end_inset

, 
\begin_inset Formula $P_{i}P_{j}=0$
\end_inset

.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $\sum_{i=1}^{n}P_{i}=I$
\end_inset

, conjugation by 
\begin_inset Formula $P_{j}$
\end_inset

 gives 
\begin_inset Formula $P_{j}\left(\sum_{i=1}^{n}P_{i}\right)P_{j}=I$
\end_inset

.
 Hence, 
\begin_inset Formula $0=P_{j}\left(\sum_{i\neq j}^{n}P_{i}\right)P_{j}=\sum_{i\neq j}^{n}P_{j}P_{i}P_{j}=\sum_{i\neq j}\left(P_{i}P_{j}\right)^{\star}\left(P_{i}P_{j}\right)$
\end_inset

.
 The sum of positive operators can only be 
\begin_inset Formula $0$
\end_inset

 if each of them is 
\begin_inset Formula $0$
\end_inset

.
 Hence, 
\begin_inset Formula $\left(P_{i}P_{j}\right)^{\star}\left(P_{i}P_{j}\right)=0$
\end_inset

 for all 
\begin_inset Formula $i\neq j$
\end_inset

.
 So 
\begin_inset Formula $P_{i}P_{j}=0$
\end_inset

.
\end_layout

\begin_layout Theorem
(Rank-Loop condition) Suppose we have a probabilistic model 
\begin_inset Formula $p$
\end_inset

.
 There exists two certificates 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 and 
\begin_inset Formula $\Gamma^{n+1}$
\end_inset

 such that 
\begin_inset Formula $\Gamma^{n+1}$
\end_inset

 restricts to 
\begin_inset Formula $\Gamma$
\end_inset

 and Rank
\begin_inset Formula $\left(\Gamma^{n+1}\right)=$
\end_inset

Rank
\begin_inset Formula $\left(\Gamma^{n}\right)$
\end_inset

 iff 
\begin_inset Formula $p$
\end_inset

 is the behavior of a finite-dimensional quantum system.
 In this case, the dimension of the system exceeds this rank.
\end_layout

\begin_layout Proof
Suppose first that our system is finite dimensional of dimension 
\begin_inset Formula $d$
\end_inset

.
 Then the vectors from which we form our Gram matrix span a space of at
 most 
\begin_inset Formula $d$
\end_inset

.
 The lemma implies that the gram matrix formed by these vectors has rank
 at most 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Proof
Conversely, suppose that we have two certificates 
\begin_inset Formula $\Gamma^{n}$
\end_inset

 and 
\begin_inset Formula $\Gamma^{n+1}$
\end_inset

 which agree on the 
\begin_inset Formula $n\times n$
\end_inset

 submatrix, and which have the same rank.
 Our Hilbert Space will be the finite dimensional space spanned by 
\begin_inset Formula $H=Span\left(\prod_{j=1}^{n}P_{j}\left|\psi\right\rangle \right)$
\end_inset

, and we will define projectors 
\begin_inset Formula $P_{i}\coloneqq Proj\left(Span\left(\left\{ P_{i}S\mid S=\prod_{j=1}^{n}P_{j}\left|\psi\right\rangle \right\} \right)\right)$
\end_inset

.
 Clearly, the 
\begin_inset Formula $P_{i}$
\end_inset

's are projectors, and the dimension of the space is Rank
\begin_inset Formula $\left(\Gamma^{n}\right)$
\end_inset

.
 To complete the proof, we must verify Definition 
\begin_inset Formula $43$
\end_inset

.
 We must establish that for any edge 
\begin_inset Formula $e$
\end_inset

, 
\begin_inset Formula $\sum_{v\in e}P_{v}=I$
\end_inset

.
 Our axioms for certificates (namely 3.)) allows us to conclude something
 very similar: 
\begin_inset Formula $\sum_{v\in e}S^{\dagger}P_{v}^{\dagger}W=S^{\dagger}W$
\end_inset

 when 
\begin_inset Formula $W,S$
\end_inset

 are of the form 
\begin_inset Formula $\prod_{j=1}^{n}P_{j}\left|\psi\right\rangle $
\end_inset

.
 And we have defined these vectors to span our Hilbert Space.
 By linearlity, we see that for any vectors 
\begin_inset Formula $\phi_{1},\phi_{2}\in H$
\end_inset

, we have 
\begin_inset Formula $\sum_{v\in e}\left\langle \phi_{1}\right|P_{v}^{\dagger}\left|\phi_{2}\right\rangle =\left\langle \phi_{1}\right|\left|\phi_{2}\right\rangle $
\end_inset

.
 This shows that 
\begin_inset Formula $\sum_{v\in e}P_{v}^{\dagger}=I$
\end_inset

.
\end_layout

\begin_layout Proof
Finally, I'd like to point out that we will recover the desired probabilities.
 Let 
\begin_inset Formula $P_{v}$
\end_inset

 be any projector as defined above.
 Choose any edge 
\begin_inset Formula $e$
\end_inset

 so 
\begin_inset Formula $\sum_{r\in e}P_{r}=I$
\end_inset

 by the previous argument.
 Then, denoting by 
\begin_inset Formula $e^{n}$
\end_inset

 the collection of sequences of length 
\begin_inset Formula $n$
\end_inset

 of projectors in 
\begin_inset Formula $e$
\end_inset

, we obtain 
\begin_inset Formula 
\[
\left\langle \psi\right|P_{v}\left|\psi\right\rangle =\left\langle \psi\right|P_{v}I^{n}\left|\psi\right\rangle =\left\langle \psi\right|P_{v}\left(\sum_{r\in e}P_{r}\right)^{n}\left|\psi\right\rangle =\sum_{r\in e^{n}}\left\langle \psi\right|P_{v}\left(\prod_{i=1}^{n}P_{r_{i}}\right)\left|\psi\right\rangle =\sum_{r\in e^{n}}\Gamma_{vr,\emptyset}^{n+1}=\Gamma_{v,\emptyset}^{n+1}
\]

\end_inset


\end_layout

\begin_layout Proof
Here, we have used axiom 3.) 
\begin_inset Formula $n$
\end_inset

 times in the last equality.
 Finally, axiom 5.) completes the argument.
\end_layout

\begin_layout Remark
A finer condition for stopping is given in 
\begin_inset CommandInset citation
LatexCommand cite
key "nav08"

\end_inset

.
\end_layout

\begin_layout Section*
Relationship to Measurment Scenarios
\end_layout

\begin_layout Standard
Given a measurement scenario, 
\begin_inset Formula $\left(X,M,O\right)$
\end_inset

, we can always draw a hypergraph, 
\begin_inset Formula $H\left(X\right)$
\end_inset

.
 The vertices of this hypergraph as assignments 
\begin_inset Formula $V\left(H\left(X\right)\right)=\left\{ O^{C}\mid C\in M\right\} $
\end_inset

.
 The hyperedges refer to those vertices which can be distinguished.
 Clearly, each context provides a hyperedge.
 By performing the measurements in a context, we distinguish between all
 the outcomes in that context.
 These edges provide a partition of the vertices into some of the hyperedges.
\end_layout

\begin_layout Standard
However, these are not all the hyperedges.
 We can obtain more by performing measurement protocols, where the choice
 of the next measurement is determined by the outcome of the previous one.
 The subsequent measurement must be compatible with the first.
 The next definition captures this compatibility.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\left(X,M,O\right)$
\end_inset

 be a measurement scenario and 
\begin_inset Formula $A\in X$
\end_inset

.
 Then the induced scenario, 
\begin_inset Formula $X\left\{ A\right\} $
\end_inset

, is the measurement scenario 
\begin_inset Formula $\left\{ X^{\prime},M^{\prime},O^{\prime}\right\} $
\end_inset

, where 
\begin_inset Formula $X^{\prime}=A^{\prime}\in X\mid A^{\prime}\neq A,\exists C\in M\mbox{ st }\left(A,A^{\prime}\right)\in C$
\end_inset

, 
\begin_inset Formula $M^{\prime}=\left\{ C\cap X^{\prime}\mid C\in M\right\} $
\end_inset

, and 
\begin_inset Formula $O^{\prime}=O$
\end_inset

.
\end_layout

\begin_layout Standard
After choosing the measurement 
\begin_inset Formula $A$
\end_inset

, our subsequent measurements are part of the induced measurement scenario,
 
\begin_inset Formula $X\left\{ A\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
Next, we define measurement protocols where subsequent measurements are
 allowed to depend on the outcomes of previous measurements.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\left(X,M,O\right)$
\end_inset

 be a measurement scenario.
 A measurement protocol, 
\begin_inset Formula $T=MP\left(X\right)$
\end_inset

, is defined recursively by
\end_layout

\begin_layout Enumerate
\begin_inset Formula $T=\emptyset$
\end_inset

 if 
\begin_inset Formula $X=\emptyset$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $T=\left(A,f\right)$
\end_inset

 where 
\begin_inset Formula $A\in X$
\end_inset

 is an observable and 
\begin_inset Formula $f:O\to MP\left(X\left\{ A\right\} \right)$
\end_inset

 is a function.
\end_layout

\begin_layout Standard
Measurement protocols can be performed, yeilding outcomes.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $T$
\end_inset

 be a measurement protocol for a scenario 
\begin_inset Formula $X$
\end_inset

.
 Then define recursively 
\begin_inset Formula $Out\left(T\right)=\left\{ \left(A,a,\alpha^{\prime}\right):a\in O,\alpha^{\prime}\in Out\left(f\left(a\right)\right)\right\} $
\end_inset

, with base case 
\begin_inset Formula $Out\left(\emptyset\right)=\left\{ \star\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
The outcome of a measurement scenario consists of actually enacting the
 plan of measurments defined by that measurement scenario.
 Thus, after all measurements have been made, we will have preformed all
 of the measurements in a context and we will have outcomes for each measurement.
 Thus, the outcome of a measurement scenario corresponds to a vertex in
 our graph.
 Each measurement scenario has a collection of outcomes, each of which correspon
d to a vertex in our graph.
 Hence, the measurement scenarios define a hyperedge.
\end_layout

\begin_layout Standard
There is some difficulty in that different measurement protocols may have
 outcomes which name the same collection of vertices.
 For example, if we choose any clique, then each ordering of the vertices
 corresponds to a different measurement protocol, where 
\begin_inset Formula $f$
\end_inset

 is chosen to be the constant function, returning the next measurement in
 that context.
\end_layout

\begin_layout Example
Suppose our measurement scenario consists of 
\begin_inset Formula $X=A,B,C$
\end_inset

; 
\begin_inset Formula $M=\left\{ AB,BC,CA\right\} $
\end_inset

; and 
\begin_inset Formula $O=\left\{ 0,1\right\} $
\end_inset

.
\end_layout

\begin_layout Example
(hand-drawn)
\end_layout

\begin_layout Problem
Suppose we start with a measurement scenario, 
\begin_inset Formula $\left(X,M,O\right)$
\end_inset

 and construct the hypergraph as above.
 Is it true that our hypergraph will be a clutter.
 If 
\begin_inset Formula $e_{1},e_{2}\in E$
\end_inset

 with 
\begin_inset Formula $e_{1}\subset e_{2}$
\end_inset

, then 
\begin_inset Formula $e_{1}=e_{2}$
\end_inset

.
\end_layout

\begin_layout Standard
Each measurement scenario gives rise to a hypergraph, but hypergraphs to
 not necessarily correspond to measurement scenarios.
 For example, consider the hypergraph on 
\begin_inset Formula $5$
\end_inset

 vertices, containing two hyperedges.
 Each hyperedge contains 
\begin_inset Formula $3$
\end_inset

 vertices, and one vertex is contained in both hyperedges.
 Since there is no collection of disjoint hyperedges which covers the vertices,
 this hypergraph cannot come from a measurement scenario.
\end_layout

\begin_layout Problem
Under what conditions does a hypergraph correspond to a measurement scenario?
 Is it sufficient to require the existence of disjoint edges which cover
 the vertices? Do these hypergraphs enjoy any special properties?
\end_layout

\begin_layout Theorem
If we have a measurement scenario which gives rise to a hypergraph, then
 empirical models on that measurement scenario are probabilistic models
 on the hypergraph and vice versa.
 The correspondence is also linear.
\end_layout

\begin_layout Proof
See 
\begin_inset CommandInset citation
LatexCommand cite
key "fri15"

\end_inset


\end_layout

\begin_layout Standard
Measurement scenarios make it easy to study non-locality.
 If we have two sites, each of which has a collection of measurements, then
 we can define the contexts consist of all pairs of measurements from each
 site.
 If our hypergraphs come from measurement scenarios, then we can use the
 correspondence to define non-local products of hypergraphs.
 Otherwise, we can define measurement protocols for pairs of hypergraphs.
 These measurement protocols will define the hyperedges in the non-local
 product of hypergraphs.
\end_layout

\begin_layout Proposition
Any possibilistic empirical model has a graph which underlies it- this graph
 connects sections when they are incompatible.
 We can also draw a hypergraph for a possibilistic empirical model using
 measurement protocols to define the edges.
 In this case, the graph underlying the hypergraph is the same as the graph
 of incompatible sections.
\end_layout

\begin_layout Proof
Suppose we have a measurement scenario 
\begin_inset Formula $\left\langle X,M,O\right\rangle $
\end_inset

 and a possibilistic empirical model, 
\begin_inset Formula $\mathscr{S}:\left(2^{X}\right)^{op}\to SET$
\end_inset

.
 Now, suppose that two vertices (sections), 
\begin_inset Formula $v_{1},v_{2}$
\end_inset

 are incompatible, in the sense that 
\begin_inset Formula $v_{1}$
\end_inset

 and 
\begin_inset Formula $v_{2}$
\end_inset

 provide different outcomes to the same measurement.
 Then we can provide a measurement protocol 
\begin_inset Formula $T$
\end_inset

 which first performs this differing measurement.
 If the outcomes is the same as that of 
\begin_inset Formula $v_{1}$
\end_inset

, then we proceed with the other measurements contained in 
\begin_inset Formula $v_{1}$
\end_inset

.
 Otherwise, we proceed with the same measurements of 
\begin_inset Formula $v_{2}$
\end_inset

.
 Clearly, 
\begin_inset Formula $v_{1}$
\end_inset

 and 
\begin_inset Formula $v_{2}$
\end_inset

 are possible outcomes of the protocol.
 Hence, the two vertices are contained in the same hyperedge, and are adjacent
 in the underlying graph.
\end_layout

\begin_layout Proof
Conversely, if 
\begin_inset Formula $v_{1}$
\end_inset

 and 
\begin_inset Formula $v_{2}$
\end_inset

 are different vertices in the same hyperedge, then they will have to be
 incompatible.
 This hyperedge corresponds to a measurement protocol where 
\begin_inset Formula $v_{1}$
\end_inset

 and 
\begin_inset Formula $v_{2}$
\end_inset

 are distinct outcomes.
\end_layout

\begin_layout Section*
Future Directions
\end_layout

\begin_layout Standard
Information Theory:
\end_layout

\begin_layout Standard
The original invention of 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 was as a bound on 
\begin_inset Formula $\Theta\left(G\right)\coloneqq\lim_{k\to\infty}\sqrt[k]{\alpha\left(G^{k}\right)}$
\end_inset

 (
\begin_inset Formula $G^{k}$
\end_inset

 is the (!)or-product), the Shannon-information capacity of a graph.
 This can be interpreted as the amount of information that can be encoded
 into an alphabet where some letters can be confused for one another.
 The graph has vertices representing the alphabet and edges which are the
 (not necessarily transitive) confusability relation.
\end_layout

\begin_layout Standard
There is a dual notion of Graph entropy.
 Rather than the graph representing the communication channel, we view the
 graph as representing the information source.
 If we consider Shannon's original problem of encoding information into
 a noisy channel, then we should also consider the problem of retrieving
 the information from a noisy source.
 We define
\begin_inset CommandInset citation
LatexCommand cite
key "sim01"

\end_inset


\begin_inset Formula 
\[
H\left(G;p\right)=\min_{a\in VP\left(G\right)}\sum_{i=1}^{n}p_{i}\log\frac{1}{a_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
wi
\end_layout

\begin_layout Standard
Computational Complexity:
\end_layout

\begin_layout Standard
Main Strategy:
\end_layout

\begin_layout Standard
The main thing that I'd like to do is identify graphs with large gaps between
 
\begin_inset Formula $\vartheta$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

, since these represent scenario
\end_layout

\begin_layout Standard
Questions:
\end_layout

\begin_layout Standard
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

 is a monotonic function on graphs of 
\begin_inset Formula $n$
\end_inset

 vertices when graph are ordered by edge-containment.
 There are some edges which can be removed (or added) without affecting
 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

.
 For example, it is known
\begin_inset CommandInset citation
LatexCommand cite
key "knu93"

\end_inset

 that 
\begin_inset Formula $\vartheta$
\end_inset

 distributes over both strong ('and') and weak('or') products of graphs,
 so 
\begin_inset Formula $\vartheta\left(G\times H\right)=\vartheta\left(G\right)\vartheta\left(H\right)=\vartheta\left(G\boxtimes H\right)$
\end_inset

 and in particular
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{\vartheta\left(G\times G\right)}=\vartheta\left(G\right)=\sqrt{\vartheta\left(G\boxtimes H\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
the edges that are in the weak product but not the strong can be removed
 or added without changing 
\begin_inset Formula $\vartheta\left(G\right)$
\end_inset

.
 Is there some way to identify all of the edges which can be added or removed?
 How does 
\begin_inset Formula $\vartheta$
\end_inset

 behave as a morphism of posets?
\end_layout

\begin_layout Standard
This may be related to a distinction between orthonormal representations
 and 
\series bold
faithful
\series default
 orthonormal representations
\begin_inset CommandInset citation
LatexCommand cite
key "lovbook"

\end_inset

, in which we require that two adjacent vertices are assigned non-orthogonal
 vectors.
 Is it true that when all optimal representations of a graph 
\begin_inset Formula $G$
\end_inset

 are faithful, no edges can be added to 
\begin_inset Formula $G$
\end_inset

 without changing its 
\begin_inset Formula $\vartheta$
\end_inset

-value? 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "contextualitybib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
